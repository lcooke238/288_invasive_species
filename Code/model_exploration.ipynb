{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Exploration\n",
    "In this file, I will be investigating the PAWS model for Stackelberg Security Games and Wildlife Conservation: https://projects.iq.harvard.edu/files/teamcore/files/project_paws_publication_01_teamcore_yang_et_al_2014.pdf. In this paper, the authors design a stackelberg game where the two agents are the rangers and the poachers. They play a turn based game where the rangers first setup a patrol strategy. The poachers can then observe this strategy and come up with a poaching strategy in return. The patrol/poaching areas are discritized by splitting the map into a grid, and a strategy is defined by assigning the limited number of rangers to a cell of the grid. From here, utility is assigned to the different outcomes of people present combined with if there are endangered animals present, with positive utility given to catching and stopping poachers and negative utility to poachers without a ranger. Using this score of the board, the rangers then update their strategy to reflect the observed poacher behavior. This cycle repeats with the goal of rangers encountering more poachers.\n",
    "\n",
    "For our approach, we want to use this same model with the agents being the trappers and the invasive species we are looking to capture. To make this modification, we first need to change the utility structure such that we reward traps capturing the invasive species and disincentivize traps being placed somewhere where they dont catch an invasive animal. To do this, we first need to understand how the original paws implementation works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAWS Structural Outline and the modifications we will need to make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From my 91r work junior fall, I have put together a rough outline of the PAWS codebase as it works as a flask app in python. To run the model, researchers do the following things:\n",
    "\n",
    "1. **Setup the Flask App and log**: Sets up the overhead and logging needed to track the execution of the flask app containing this software. This portion also establishes connection to an Azure server containing the data needed to operate the program. This portion, relevant for deployment and scaling, will not be relevant to our implementation.\n",
    "2. **Data Input and Breakdown**: In the PAWS design, researchers take data keys from a JSON file that then allow them to access the data of interest from the cloud. This data comes in 3 parts:\n",
    "    - shapes, or the shapefiles representing the patrolled park (i.e. park boundaries or locations of different features of the landscape like rivers and roads)\n",
    "    - rasters, or the raster files providing supplemental information about a patrolled park (i.e. elevation of different areas in a park, land cover)\n",
    "    - patrol_observations, or a csv containing information about the different patrols used for prediction.\n",
    "    \n",
    "    I have a data dictionary for both the JSON file and patrol_observations file to understand what data goes where. Generally though, for our implementation we will be cutting the JSON mapping to the cloud and grabbing the right datasets and instead creating a function to setup the data we need locally.\n",
    "\n",
    "3. **Data Validation and Error Handling**: this section allows us to verify that necessary features of a given piece of data holds before attempting to process the data for prediction. Problems are broken down into Warnings, Errors, and FatalErrors that will log and kill the program if fatal. Understanding and updating these checks based on the modified reward and data will be important as we test and develop our modifications.\n",
    "4. **Preprocess**: This does all of the data pre-processing, where we grab the boundary, clean the data, get shapefiles, distances, and rasters, calculate effort, and compute illegal activity. Some data verification does happen in this file, so checking if this data verification can be refactored into the data validation section could be useful. For our implementation, I think the preprocessing will look quite similar with the exception of how to process the reward-defining activity.\n",
    "5. **Consolidate**: This consolidates all of the data pieces into a single dataframe for the model to use. The PAWS implementaton of consolidate contains some repeated processes that already exist in preprocess, so for our implementation it will be nice to remove those repeat processes. Besides this, consolidation should look extremely similar for our implementation. Additionally, it may be worth working with everything in a more final format from the beginning to avoid unnecessary overhead, but this would be future work after an initial implementation.\n",
    "6. **Predict**: Here, taking this single dataframe, we ultimately predict the locations for our next round of patrols. For our implementation, the prediction should be identical, so long as the modified reward is captured within the data. If not, then the actual prediction schematic may need to update its scoring to reflect rewarding the successful traps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invasive Species Modifications\n",
    "\n",
    "Working on this, I want to start with the initial framework of the prediction to better understand what it uses and therefore what it needs for modifications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## log handling ##\n",
    "\n",
    "class Log:\n",
    "    def __init__(self, filename=\"../Logs/log.txt\"):\n",
    "        self.filename = filename\n",
    "        # Clear the file's contents if it exists\n",
    "        with open(self.filename, \"w\") as file:\n",
    "            pass\n",
    "\n",
    "    def write_line(self, line):\n",
    "        with open(self.filename, \"a\") as file:\n",
    "            file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data input and breakdown ##\n",
    "\n",
    "def traps_get_data():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data validation and error handling ##\n",
    "\n",
    "def traps_validate_data():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess ##\n",
    "\n",
    "def traps_preprocess():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## consolidate ##\n",
    "\n",
    "def traps_consolidate():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict ##\n",
    "\n",
    "# define model class to train and predict from\n",
    "\n",
    "def traps_predict():\n",
    "    # define helper that runs predictions based on an input month type (dry/rainy)\n",
    "    \n",
    "    # To predict:\n",
    "        # find number of sections\n",
    "        # gather the data into its final format:\n",
    "            # get post-processed data csv (raw features)\n",
    "            # get post-processed labels (raw labels)\n",
    "            # generate the following:\n",
    "                # patrol_effort: features_raw['current_patrol_effort'].values, direct value from raw features csv\n",
    "                # section_col: features_raw['section'].values, direct value from raw features csv\n",
    "                # features_raw: overwrite pulled raw features csv that cuts the current patrol effort and the first column (TODO what is that first column, perhaps empty overhead?)\n",
    "                # features: copy of overwritten features_raw that removes global_id, year, section, spatial_id, x, y and only contains the values of the remaining content\n",
    "                # feature_names: object that contains the names of all the remaining values captured in features\n",
    "                # labels: modified original labels that removes the first column, global_id, year, section, spatial_id, x, y, and only contains the values of the remaining content\n",
    "        # get integer classifier count, i believe from os (int(getenv('NUM_CLASSIFIERS')))\n",
    "        # create model class instance\n",
    "        # get static features\n",
    "        # train the model\n",
    "        # iterate over number of sections:\n",
    "            # find month range\n",
    "            # log and make predictions for given month range\n",
    "            # add predictions to predictions directory\n",
    "            # add variances of those predictions to the write directory if desired\n",
    "\n",
    "    # run overall predictions for all months\n",
    "\n",
    "    # check if certain type of month is asked for, if so run the predictions just for those types of months\n",
    "\n",
    "    # log completion\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## full API function ##\n",
    "\n",
    "def invasive_species_trap_placement_api(log):\n",
    "    data = traps_get_data()\n",
    "    log.write_line(\"Finished grabbing data.\")\n",
    "    \n",
    "    validated_data = traps_validate_data()\n",
    "    log.write_line(\"Finished validating data.\")\n",
    "\n",
    "    preprocessed_data = traps_preprocess()\n",
    "    log.write_line(\"Finished preprocessing data.\")\n",
    "\n",
    "    consolidated_data = traps_consolidate()\n",
    "    log.write_line(\"Finished consolidating data.\")\n",
    "\n",
    "    predictions = traps_predict()\n",
    "    log.write_line(\"Finished predicting trap locations.\")\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the code ##\n",
    "\n",
    "# setup log\n",
    "log = Log(\"../Logs/log.txt\")\n",
    "log.write_line(\"Initialized log.\")\n",
    "\n",
    "# run pipeline\n",
    "predictions = invasive_species_trap_placement_api(log)\n",
    "log.write_line(\"Finished predicting, closing log.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAWS Implementations To Annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GAUSSIAN PROCESS CLASSIFICATION FROM PAWS ##\n",
    "\n",
    "\"\"\"Gaussian processes classification.\"\"\"\n",
    "\n",
    "# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import warnings\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import cholesky, cho_solve, solve\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from scipy.special import erf, expit\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.gaussian_process.kernels \\\n",
    "    import RBF, CompoundKernel, ConstantKernel as C\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted, check_array\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "\n",
    "# Values required for approximating the logistic sigmoid by\n",
    "# error functions. coefs are obtained via:\n",
    "# x = np.array([0, 0.6, 2, 3.5, 4.5, np.inf])\n",
    "# b = logistic(x)\n",
    "# A = (erf(np.dot(x, self.lambdas)) + 1) / 2\n",
    "# coefs = lstsq(A, b)[0]\n",
    "LAMBDAS = np.array([0.41, 0.4, 0.37, 0.44, 0.39])[:, np.newaxis]\n",
    "COEFS = np.array([-1854.8214151, 3516.89893646, 221.29346712,\n",
    "                  128.12323805, -2010.49422654])[:, np.newaxis]\n",
    "\n",
    "\n",
    "class _BinaryGaussianProcessClassifierLaplace(BaseEstimator):\n",
    "    \"\"\"Binary Gaussian process classification based on Laplace approximation.\n",
    "    The implementation is based on Algorithm 3.1, 3.2, and 5.1 of\n",
    "    ``Gaussian Processes for Machine Learning'' (GPML) by Rasmussen and\n",
    "    Williams.\n",
    "    Internally, the Laplace approximation is used for approximating the\n",
    "    non-Gaussian posterior by a Gaussian.\n",
    "    Currently, the implementation is restricted to using the logistic link\n",
    "    function.\n",
    "    .. versionadded:: 0.18\n",
    "    Parameters\n",
    "    ----------\n",
    "    kernel : kernel object\n",
    "        The kernel specifying the covariance function of the GP. If None is\n",
    "        passed, the kernel \"1.0 * RBF(1.0)\" is used as default. Note that\n",
    "        the kernel's hyperparameters are optimized during fitting.\n",
    "    optimizer : string or callable, optional (default: \"fmin_l_bfgs_b\")\n",
    "        Can either be one of the internally supported optimizers for optimizing\n",
    "        the kernel's parameters, specified by a string, or an externally\n",
    "        defined optimizer passed as a callable. If a callable is passed, it\n",
    "        must have the  signature::\n",
    "            def optimizer(obj_func, initial_theta, bounds):\n",
    "                # * 'obj_func' is the objective function to be maximized, which\n",
    "                #   takes the hyperparameters theta as parameter and an\n",
    "                #   optional flag eval_gradient, which determines if the\n",
    "                #   gradient is returned additionally to the function value\n",
    "                # * 'initial_theta': the initial value for theta, which can be\n",
    "                #   used by local optimizers\n",
    "                # * 'bounds': the bounds on the values of theta\n",
    "                ....\n",
    "                # Returned are the best found hyperparameters theta and\n",
    "                # the corresponding value of the target function.\n",
    "                return theta_opt, func_min\n",
    "        Per default, the 'fmin_l_bfgs_b' algorithm from scipy.optimize\n",
    "        is used. If None is passed, the kernel's parameters are kept fixed.\n",
    "        Available internal optimizers are::\n",
    "            'fmin_l_bfgs_b'\n",
    "    n_restarts_optimizer: int, optional (default: 0)\n",
    "        The number of restarts of the optimizer for finding the kernel's\n",
    "        parameters which maximize the log-marginal likelihood. The first run\n",
    "        of the optimizer is performed from the kernel's initial parameters,\n",
    "        the remaining ones (if any) from thetas sampled log-uniform randomly\n",
    "        from the space of allowed theta-values. If greater than 0, all bounds\n",
    "        must be finite. Note that n_restarts_optimizer=0 implies that one\n",
    "        run is performed.\n",
    "    max_iter_predict: int, optional (default: 100)\n",
    "        The maximum number of iterations in Newton's method for approximating\n",
    "        the posterior during predict. Smaller values will reduce computation\n",
    "        time at the cost of worse results.\n",
    "    warm_start : bool, optional (default: False)\n",
    "        If warm-starts are enabled, the solution of the last Newton iteration\n",
    "        on the Laplace approximation of the posterior mode is used as\n",
    "        initialization for the next call of _posterior_mode(). This can speed\n",
    "        up convergence when _posterior_mode is called several times on similar\n",
    "        problems as in hyperparameter optimization. See :term:`the Glossary\n",
    "        <warm_start>`.\n",
    "    copy_X_train : bool, optional (default: True)\n",
    "        If True, a persistent copy of the training data is stored in the\n",
    "        object. Otherwise, just a reference to the training data is stored,\n",
    "        which might cause predictions to change if the data is modified\n",
    "        externally.\n",
    "    random_state : int, RandomState instance or None, optional (default: None)\n",
    "        The generator used to initialize the centers. If int, random_state is\n",
    "        the seed used by the random number generator; If RandomState instance,\n",
    "        random_state is the random number generator; If None, the random number\n",
    "        generator is the RandomState instance used by `np.random`.\n",
    "    Attributes\n",
    "    ----------\n",
    "    X_train_ : array-like, shape = (n_samples, n_features)\n",
    "        Feature values in training data (also required for prediction)\n",
    "    y_train_ : array-like, shape = (n_samples,)\n",
    "        Target values in training data (also required for prediction)\n",
    "    classes_ : array-like, shape = (n_classes,)\n",
    "        Unique class labels.\n",
    "    kernel_ : kernel object\n",
    "        The kernel used for prediction. The structure of the kernel is the\n",
    "        same as the one passed as parameter but with optimized hyperparameters\n",
    "    L_ : array-like, shape = (n_samples, n_samples)\n",
    "        Lower-triangular Cholesky decomposition of the kernel in X_train_\n",
    "    pi_ : array-like, shape = (n_samples,)\n",
    "        The probabilities of the positive class for the training points\n",
    "        X_train_\n",
    "    W_sr_ : array-like, shape = (n_samples,)\n",
    "        Square root of W, the Hessian of log-likelihood of the latent function\n",
    "        values for the observed labels. Since W is diagonal, only the diagonal\n",
    "        of sqrt(W) is stored.\n",
    "    log_marginal_likelihood_value_ : float\n",
    "        The log-marginal-likelihood of ``self.kernel_.theta``\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel=None, optimizer=\"fmin_l_bfgs_b\",\n",
    "                 n_restarts_optimizer=0, max_iter_predict=100,\n",
    "                 warm_start=False, copy_X_train=True, random_state=None):\n",
    "        self.kernel = kernel\n",
    "        self.optimizer = optimizer\n",
    "        self.n_restarts_optimizer = n_restarts_optimizer\n",
    "        self.max_iter_predict = max_iter_predict\n",
    "        self.warm_start = warm_start\n",
    "        self.copy_X_train = copy_X_train\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit Gaussian process classification model\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape = (n_samples,)\n",
    "            Target values, must be binary\n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "        if self.kernel is None:  # Use an RBF kernel as default\n",
    "            self.kernel_ = C(1.0, constant_value_bounds=\"fixed\") \\\n",
    "                * RBF(1.0, length_scale_bounds=\"fixed\")\n",
    "        else:\n",
    "            self.kernel_ = clone(self.kernel)\n",
    "\n",
    "        self.rng = check_random_state(self.random_state)\n",
    "\n",
    "        self.X_train_ = np.copy(X) if self.copy_X_train else X\n",
    "\n",
    "        # Encode class labels and check that it is a binary classification\n",
    "        # problem\n",
    "        label_encoder = LabelEncoder()\n",
    "        self.y_train_ = label_encoder.fit_transform(y)\n",
    "        self.classes_ = label_encoder.classes_\n",
    "        if self.classes_.size > 2:\n",
    "            raise ValueError(\"%s supports only binary classification. \"\n",
    "                             \"y contains classes %s\"\n",
    "                             % (self.__class__.__name__, self.classes_))\n",
    "        elif self.classes_.size == 1:\n",
    "            raise ValueError(\"{0:s} requires 2 classes; got {1:d} class\"\n",
    "                             .format(self.__class__.__name__,\n",
    "                                     self.classes_.size))\n",
    "\n",
    "        if self.optimizer is not None and self.kernel_.n_dims > 0:\n",
    "            # Choose hyperparameters based on maximizing the log-marginal\n",
    "            # likelihood (potentially starting from several initial values)\n",
    "            def obj_func(theta, eval_gradient=True):\n",
    "                if eval_gradient:\n",
    "                    lml, grad = self.log_marginal_likelihood(\n",
    "                        theta, eval_gradient=True)\n",
    "                    return -lml, -grad\n",
    "                else:\n",
    "                    return -self.log_marginal_likelihood(theta)\n",
    "\n",
    "            # First optimize starting from theta specified in kernel\n",
    "            optima = [self._constrained_optimization(obj_func,\n",
    "                                                     self.kernel_.theta,\n",
    "                                                     self.kernel_.bounds)]\n",
    "\n",
    "            # Additional runs are performed from log-uniform chosen initial\n",
    "            # theta\n",
    "            if self.n_restarts_optimizer > 0:\n",
    "                if not np.isfinite(self.kernel_.bounds).all():\n",
    "                    raise ValueError(\n",
    "                        \"Multiple optimizer restarts (n_restarts_optimizer>0) \"\n",
    "                        \"requires that all bounds are finite.\")\n",
    "                bounds = self.kernel_.bounds\n",
    "                for iteration in range(self.n_restarts_optimizer):\n",
    "                    theta_initial = np.exp(self.rng.uniform(bounds[:, 0],\n",
    "                                                            bounds[:, 1]))\n",
    "                    optima.append(\n",
    "                        self._constrained_optimization(obj_func, theta_initial,\n",
    "                                                       bounds))\n",
    "            # Select result from run with minimal (negative) log-marginal\n",
    "            # likelihood\n",
    "            lml_values = list(map(itemgetter(1), optima))\n",
    "            self.kernel_.theta = optima[np.argmin(lml_values)][0]\n",
    "            self.log_marginal_likelihood_value_ = -np.min(lml_values)\n",
    "        else:\n",
    "            self.log_marginal_likelihood_value_ = \\\n",
    "                self.log_marginal_likelihood(self.kernel_.theta)\n",
    "\n",
    "        # Precompute quantities required for predictions which are independent\n",
    "        # of actual query points\n",
    "        K = self.kernel_(self.X_train_)\n",
    "\n",
    "        _, (self.pi_, self.W_sr_, self.L_, _, _) = \\\n",
    "            self._posterior_mode(K, return_temporaries=True)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Perform classification on an array of test vectors X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        C : array, shape = (n_samples,)\n",
    "            Predicted target values for X, values are from ``classes_``\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, [\"X_train_\", \"y_train_\", \"pi_\", \"W_sr_\", \"L_\"])\n",
    "\n",
    "        # As discussed on Section 3.4.2 of GPML, for making hard binary\n",
    "        # decisions, it is enough to compute the MAP of the posterior and\n",
    "        # pass it through the link function\n",
    "        K_star = self.kernel_(self.X_train_, X)  # K_star =k(x_star)\n",
    "        f_star = K_star.T.dot(self.y_train_ - self.pi_)  # Algorithm 3.2,Line 4\n",
    "\n",
    "        return np.where(f_star > 0, self.classes_[1], self.classes_[0])\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return probability estimates for the test vector X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        C : array-like, shape = (n_samples, n_classes)\n",
    "            Returns the probability of the samples for each class in\n",
    "            the model. The columns correspond to the classes in sorted\n",
    "            order, as they appear in the attribute ``classes_``.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, [\"X_train_\", \"y_train_\", \"pi_\", \"W_sr_\", \"L_\"])\n",
    "\n",
    "        # Based on Algorithm 3.2 of GPML\n",
    "        K_star = self.kernel_(self.X_train_, X)  # K_star =k(x_star)\n",
    "        f_star = K_star.T.dot(self.y_train_ - self.pi_)  # Line 4\n",
    "        v = solve(self.L_, self.W_sr_[:, np.newaxis] * K_star)  # Line 5\n",
    "        # Line 6 (compute np.diag(v.T.dot(v)) via einsum)\n",
    "        var_f_star = self.kernel_.diag(X) - np.einsum(\"ij,ij->j\", v, v)\n",
    "\n",
    "        # Line 7:\n",
    "        # Approximate \\int log(z) * N(z | f_star, var_f_star)\n",
    "        # Approximation is due to Williams & Barber, \"Bayesian Classification\n",
    "        # with Gaussian Processes\", Appendix A: Approximate the logistic\n",
    "        # sigmoid by a linear combination of 5 error functions.\n",
    "        # For information on how this integral can be computed see\n",
    "        # blitiri.blogspot.de/2012/11/gaussian-integral-of-error-function.html\n",
    "        alpha = 1 / (2 * var_f_star)\n",
    "        gamma = LAMBDAS * f_star\n",
    "        integrals = np.sqrt(np.pi / alpha) \\\n",
    "            * erf(gamma * np.sqrt(alpha / (alpha + LAMBDAS**2))) \\\n",
    "            / (2 * np.sqrt(var_f_star * 2 * np.pi))\n",
    "        pi_star = (COEFS * integrals).sum(axis=0) + .5 * COEFS.sum()\n",
    "\n",
    "        return np.vstack((1 - pi_star, pi_star)).T\n",
    "\n",
    "\n",
    "    def predict_var(self, X):\n",
    "        \"\"\"Return probability estimates for the test vector X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        C : array-like, shape = (n_samples, n_classes)\n",
    "            Returns the probability of the samples for each class in\n",
    "            the model. The columns correspond to the classes in sorted\n",
    "            order, as they appear in the attribute ``classes_``.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, [\"X_train_\", \"y_train_\", \"pi_\", \"W_sr_\", \"L_\"])\n",
    "\n",
    "        # Based on Algorithm 3.2 of GPML\n",
    "        K_star = self.kernel_(self.X_train_, X)  # K_star =k(x_star)\n",
    "        f_star = K_star.T.dot(self.y_train_ - self.pi_)  # Line 4\n",
    "        v = solve(self.L_, self.W_sr_[:, np.newaxis] * K_star)  # Line 5\n",
    "        # Line 6 (compute np.diag(v.T.dot(v)) via einsum)\n",
    "        var_f_star = self.kernel_.diag(X) - np.einsum(\"ij,ij->j\", v, v)\n",
    "\n",
    "        return var_f_star\n",
    "        # # Line 7:\n",
    "        # # Approximate \\int log(z) * N(z | f_star, var_f_star)\n",
    "        # # Approximation is due to Williams & Barber, \"Bayesian Classification\n",
    "        # # with Gaussian Processes\", Appendix A: Approximate the logistic\n",
    "        # # sigmoid by a linear combination of 5 error functions.\n",
    "        # # For information on how this integral can be computed see\n",
    "        # # blitiri.blogspot.de/2012/11/gaussian-integral-of-error-function.html\n",
    "        # alpha = 1 / (2 * var_f_star)\n",
    "        # gamma = LAMBDAS * f_star\n",
    "        # integrals = np.sqrt(np.pi / alpha) \\\n",
    "        #     * erf(gamma * np.sqrt(alpha / (alpha + LAMBDAS**2))) \\\n",
    "        #     / (2 * np.sqrt(var_f_star * 2 * np.pi))\n",
    "        # pi_star = (COEFS * integrals).sum(axis=0) + .5 * COEFS.sum()\n",
    "\n",
    "        # return np.vstack((1 - pi_star, pi_star)).T\n",
    "\n",
    "\n",
    "    def log_marginal_likelihood(self, theta=None, eval_gradient=False):\n",
    "        \"\"\"Returns log-marginal likelihood of theta for training data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        theta : array-like, shape = (n_kernel_params,) or None\n",
    "            Kernel hyperparameters for which the log-marginal likelihood is\n",
    "            evaluated. If None, the precomputed log_marginal_likelihood\n",
    "            of ``self.kernel_.theta`` is returned.\n",
    "        eval_gradient : bool, default: False\n",
    "            If True, the gradient of the log-marginal likelihood with respect\n",
    "            to the kernel hyperparameters at position theta is returned\n",
    "            additionally. If True, theta must not be None.\n",
    "        Returns\n",
    "        -------\n",
    "        log_likelihood : float\n",
    "            Log-marginal likelihood of theta for training data.\n",
    "        log_likelihood_gradient : array, shape = (n_kernel_params,), optional\n",
    "            Gradient of the log-marginal likelihood with respect to the kernel\n",
    "            hyperparameters at position theta.\n",
    "            Only returned when eval_gradient is True.\n",
    "        \"\"\"\n",
    "        if theta is None:\n",
    "            if eval_gradient:\n",
    "                raise ValueError(\n",
    "                    \"Gradient can only be evaluated for theta!=None\")\n",
    "            return self.log_marginal_likelihood_value_\n",
    "\n",
    "        kernel = self.kernel_.clone_with_theta(theta)\n",
    "\n",
    "        if eval_gradient:\n",
    "            K, K_gradient = kernel(self.X_train_, eval_gradient=True)\n",
    "        else:\n",
    "            K = kernel(self.X_train_)\n",
    "\n",
    "        # Compute log-marginal-likelihood Z and also store some temporaries\n",
    "        # which can be reused for computing Z's gradient\n",
    "        Z, (pi, W_sr, L, b, a) = \\\n",
    "            self._posterior_mode(K, return_temporaries=True)\n",
    "\n",
    "        if not eval_gradient:\n",
    "            return Z\n",
    "\n",
    "        # Compute gradient based on Algorithm 5.1 of GPML\n",
    "        d_Z = np.empty(theta.shape[0])\n",
    "        # XXX: Get rid of the np.diag() in the next line\n",
    "        R = W_sr[:, np.newaxis] * cho_solve((L, True), np.diag(W_sr))  # Line 7\n",
    "        C = solve(L, W_sr[:, np.newaxis] * K)  # Line 8\n",
    "        # Line 9: (use einsum to compute np.diag(C.T.dot(C))))\n",
    "        s_2 = -0.5 * (np.diag(K) - np.einsum('ij, ij -> j', C, C)) \\\n",
    "            * (pi * (1 - pi) * (1 - 2 * pi))  # third derivative\n",
    "\n",
    "        for j in range(d_Z.shape[0]):\n",
    "            C = K_gradient[:, :, j]   # Line 11\n",
    "            # Line 12: (R.T.ravel().dot(C.ravel()) = np.trace(R.dot(C)))\n",
    "            s_1 = .5 * a.T.dot(C).dot(a) - .5 * R.T.ravel().dot(C.ravel())\n",
    "\n",
    "            b = C.dot(self.y_train_ - pi)  # Line 13\n",
    "            s_3 = b - K.dot(R.dot(b))  # Line 14\n",
    "\n",
    "            d_Z[j] = s_1 + s_2.T.dot(s_3)  # Line 15\n",
    "\n",
    "        return Z, d_Z\n",
    "\n",
    "    def _posterior_mode(self, K, return_temporaries=False):\n",
    "        \"\"\"Mode-finding for binary Laplace GPC and fixed kernel.\n",
    "        This approximates the posterior of the latent function values for given\n",
    "        inputs and target observations with a Gaussian approximation and uses\n",
    "        Newton's iteration to find the mode of this approximation.\n",
    "        \"\"\"\n",
    "        # Based on Algorithm 3.1 of GPML\n",
    "\n",
    "        # If warm_start are enabled, we reuse the last solution for the\n",
    "        # posterior mode as initialization; otherwise, we initialize with 0\n",
    "        if self.warm_start and hasattr(self, \"f_cached\") \\\n",
    "           and self.f_cached.shape == self.y_train_.shape:\n",
    "            f = self.f_cached\n",
    "        else:\n",
    "            f = np.zeros_like(self.y_train_, dtype=np.float64)\n",
    "\n",
    "        # Use Newton's iteration method to find mode of Laplace approximation\n",
    "        log_marginal_likelihood = -np.inf\n",
    "        for _ in range(self.max_iter_predict):\n",
    "            # Line 4\n",
    "            pi = expit(f)\n",
    "            W = pi * (1 - pi)\n",
    "            # Line 5\n",
    "            W_sr = np.sqrt(W)\n",
    "            W_sr_K = W_sr[:, np.newaxis] * K\n",
    "            B = np.eye(W.shape[0]) + W_sr_K * W_sr\n",
    "            L = cholesky(B, lower=True)\n",
    "            # Line 6\n",
    "            b = W * f + (self.y_train_ - pi)\n",
    "            # Line 7\n",
    "            a = b - W_sr * cho_solve((L, True), W_sr_K.dot(b))\n",
    "            # Line 8\n",
    "            f = K.dot(a)\n",
    "\n",
    "            # Line 10: Compute log marginal likelihood in loop and use as\n",
    "            #          convergence criterion\n",
    "            lml = -0.5 * a.T.dot(f) \\\n",
    "                - np.log1p(np.exp(-(self.y_train_ * 2 - 1) * f)).sum() \\\n",
    "                - np.log(np.diag(L)).sum()\n",
    "            # Check if we have converged (log marginal likelihood does\n",
    "            # not decrease)\n",
    "            # XXX: more complex convergence criterion\n",
    "            if lml - log_marginal_likelihood < 1e-10:\n",
    "                break\n",
    "            log_marginal_likelihood = lml\n",
    "\n",
    "        self.f_cached = f  # Remember solution for later warm-starts\n",
    "        if return_temporaries:\n",
    "            return log_marginal_likelihood, (pi, W_sr, L, b, a)\n",
    "        else:\n",
    "            return log_marginal_likelihood\n",
    "\n",
    "    def _constrained_optimization(self, obj_func, initial_theta, bounds):\n",
    "        if self.optimizer == \"fmin_l_bfgs_b\":\n",
    "            theta_opt, func_min, convergence_dict = \\\n",
    "                fmin_l_bfgs_b(obj_func, initial_theta, bounds=bounds)\n",
    "            if convergence_dict[\"warnflag\"] != 0:\n",
    "                warnings.warn(\"fmin_l_bfgs_b terminated abnormally with the \"\n",
    "                              \" state: %s\" % convergence_dict,\n",
    "                              ConvergenceWarning)\n",
    "        elif callable(self.optimizer):\n",
    "            theta_opt, func_min = \\\n",
    "                self.optimizer(obj_func, initial_theta, bounds=bounds)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown optimizer %s.\" % self.optimizer)\n",
    "\n",
    "        return theta_opt, func_min\n",
    "\n",
    "\n",
    "class GaussianProcessClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Gaussian process classification (GPC) based on Laplace approximation.\n",
    "    The implementation is based on Algorithm 3.1, 3.2, and 5.1 of\n",
    "    Gaussian Processes for Machine Learning (GPML) by Rasmussen and\n",
    "    Williams.\n",
    "    Internally, the Laplace approximation is used for approximating the\n",
    "    non-Gaussian posterior by a Gaussian.\n",
    "    Currently, the implementation is restricted to using the logistic link\n",
    "    function. For multi-class classification, several binary one-versus rest\n",
    "    classifiers are fitted. Note that this class thus does not implement\n",
    "    a true multi-class Laplace approximation.\n",
    "    Parameters\n",
    "    ----------\n",
    "    kernel : kernel object\n",
    "        The kernel specifying the covariance function of the GP. If None is\n",
    "        passed, the kernel \"1.0 * RBF(1.0)\" is used as default. Note that\n",
    "        the kernel's hyperparameters are optimized during fitting.\n",
    "    optimizer : string or callable, optional (default: \"fmin_l_bfgs_b\")\n",
    "        Can either be one of the internally supported optimizers for optimizing\n",
    "        the kernel's parameters, specified by a string, or an externally\n",
    "        defined optimizer passed as a callable. If a callable is passed, it\n",
    "        must have the  signature::\n",
    "            def optimizer(obj_func, initial_theta, bounds):\n",
    "                # * 'obj_func' is the objective function to be maximized, which\n",
    "                #   takes the hyperparameters theta as parameter and an\n",
    "                #   optional flag eval_gradient, which determines if the\n",
    "                #   gradient is returned additionally to the function value\n",
    "                # * 'initial_theta': the initial value for theta, which can be\n",
    "                #   used by local optimizers\n",
    "                # * 'bounds': the bounds on the values of theta\n",
    "                ....\n",
    "                # Returned are the best found hyperparameters theta and\n",
    "                # the corresponding value of the target function.\n",
    "                return theta_opt, func_min\n",
    "        Per default, the 'fmin_l_bfgs_b' algorithm from scipy.optimize\n",
    "        is used. If None is passed, the kernel's parameters are kept fixed.\n",
    "        Available internal optimizers are::\n",
    "            'fmin_l_bfgs_b'\n",
    "    n_restarts_optimizer : int, optional (default: 0)\n",
    "        The number of restarts of the optimizer for finding the kernel's\n",
    "        parameters which maximize the log-marginal likelihood. The first run\n",
    "        of the optimizer is performed from the kernel's initial parameters,\n",
    "        the remaining ones (if any) from thetas sampled log-uniform randomly\n",
    "        from the space of allowed theta-values. If greater than 0, all bounds\n",
    "        must be finite. Note that n_restarts_optimizer=0 implies that one\n",
    "        run is performed.\n",
    "    max_iter_predict : int, optional (default: 100)\n",
    "        The maximum number of iterations in Newton's method for approximating\n",
    "        the posterior during predict. Smaller values will reduce computation\n",
    "        time at the cost of worse results.\n",
    "    warm_start : bool, optional (default: False)\n",
    "        If warm-starts are enabled, the solution of the last Newton iteration\n",
    "        on the Laplace approximation of the posterior mode is used as\n",
    "        initialization for the next call of _posterior_mode(). This can speed\n",
    "        up convergence when _posterior_mode is called several times on similar\n",
    "        problems as in hyperparameter optimization. See :term:`the Glossary\n",
    "        <warm_start>`.\n",
    "    copy_X_train : bool, optional (default: True)\n",
    "        If True, a persistent copy of the training data is stored in the\n",
    "        object. Otherwise, just a reference to the training data is stored,\n",
    "        which might cause predictions to change if the data is modified\n",
    "        externally.\n",
    "    random_state : int, RandomState instance or None, optional (default: None)\n",
    "        The generator used to initialize the centers.\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "    multi_class : string, default : \"one_vs_rest\"\n",
    "        Specifies how multi-class classification problems are handled.\n",
    "        Supported are \"one_vs_rest\" and \"one_vs_one\". In \"one_vs_rest\",\n",
    "        one binary Gaussian process classifier is fitted for each class, which\n",
    "        is trained to separate this class from the rest. In \"one_vs_one\", one\n",
    "        binary Gaussian process classifier is fitted for each pair of classes,\n",
    "        which is trained to separate these two classes. The predictions of\n",
    "        these binary predictors are combined into multi-class predictions.\n",
    "        Note that \"one_vs_one\" does not support predicting probability\n",
    "        estimates.\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        The number of jobs to use for the computation.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "    Attributes\n",
    "    ----------\n",
    "    kernel_ : kernel object\n",
    "        The kernel used for prediction. In case of binary classification,\n",
    "        the structure of the kernel is the same as the one passed as parameter\n",
    "        but with optimized hyperparameters. In case of multi-class\n",
    "        classification, a CompoundKernel is returned which consists of the\n",
    "        different kernels used in the one-versus-rest classifiers.\n",
    "    log_marginal_likelihood_value_ : float\n",
    "        The log-marginal-likelihood of ``self.kernel_.theta``\n",
    "    classes_ : array-like, shape = (n_classes,)\n",
    "        Unique class labels.\n",
    "    n_classes_ : int\n",
    "        The number of classes in the training data\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_iris\n",
    "    >>> from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "    >>> from sklearn.gaussian_process.kernels import RBF\n",
    "    >>> X, y = load_iris(return_X_y=True)\n",
    "    >>> kernel = 1.0 * RBF(1.0)\n",
    "    >>> gpc = GaussianProcessClassifier(kernel=kernel,\n",
    "    ...         random_state=0).fit(X, y)\n",
    "    >>> gpc.score(X, y) # doctest: +ELLIPSIS\n",
    "    0.9866...\n",
    "    >>> gpc.predict_proba(X[:2,:])\n",
    "    array([[0.83548752, 0.03228706, 0.13222543],\n",
    "           [0.79064206, 0.06525643, 0.14410151]])\n",
    "    .. versionadded:: 0.18\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel=None, optimizer=\"fmin_l_bfgs_b\",\n",
    "                 n_restarts_optimizer=0, max_iter_predict=100,\n",
    "                 warm_start=False, copy_X_train=True, random_state=None,\n",
    "                 multi_class=\"one_vs_rest\", n_jobs=None):\n",
    "        self.kernel = kernel\n",
    "        self.optimizer = optimizer\n",
    "        self.n_restarts_optimizer = n_restarts_optimizer\n",
    "        self.max_iter_predict = max_iter_predict\n",
    "        self.warm_start = warm_start\n",
    "        self.copy_X_train = copy_X_train\n",
    "        self.random_state = random_state\n",
    "        self.multi_class = multi_class\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit Gaussian process classification model\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape = (n_samples,)\n",
    "            Target values, must be binary\n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "        X, y = check_X_y(X, y, multi_output=False)\n",
    "\n",
    "        self.base_estimator_ = _BinaryGaussianProcessClassifierLaplace(\n",
    "            self.kernel, self.optimizer, self.n_restarts_optimizer,\n",
    "            self.max_iter_predict, self.warm_start, self.copy_X_train,\n",
    "            self.random_state)\n",
    "\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_classes_ = self.classes_.size\n",
    "        if self.n_classes_ == 1:\n",
    "            raise ValueError(\"GaussianProcessClassifier requires 2 or more \"\n",
    "                             \"distinct classes; got %d class (only class %s \"\n",
    "                             \"is present)\"\n",
    "                             % (self.n_classes_, self.classes_[0]))\n",
    "        if self.n_classes_ > 2:\n",
    "            if self.multi_class == \"one_vs_rest\":\n",
    "                self.base_estimator_ = \\\n",
    "                    OneVsRestClassifier(self.base_estimator_,\n",
    "                                        n_jobs=self.n_jobs)\n",
    "            elif self.multi_class == \"one_vs_one\":\n",
    "                self.base_estimator_ = \\\n",
    "                    OneVsOneClassifier(self.base_estimator_,\n",
    "                                       n_jobs=self.n_jobs)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown multi-class mode %s\"\n",
    "                                 % self.multi_class)\n",
    "\n",
    "        self.base_estimator_.fit(X, y)\n",
    "\n",
    "        if self.n_classes_ > 2:\n",
    "            self.log_marginal_likelihood_value_ = np.mean(\n",
    "                [estimator.log_marginal_likelihood()\n",
    "                 for estimator in self.base_estimator_.estimators_])\n",
    "        else:\n",
    "            self.log_marginal_likelihood_value_ = \\\n",
    "                self.base_estimator_.log_marginal_likelihood()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Perform classification on an array of test vectors X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        C : array, shape = (n_samples,)\n",
    "            Predicted target values for X, values are from ``classes_``\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, [\"classes_\", \"n_classes_\"])\n",
    "        X = check_array(X)\n",
    "        return self.base_estimator_.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return probability estimates for the test vector X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        C : array-like, shape = (n_samples, n_classes)\n",
    "            Returns the probability of the samples for each class in\n",
    "            the model. The columns correspond to the classes in sorted\n",
    "            order, as they appear in the attribute `classes_`.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, [\"classes_\", \"n_classes_\"])\n",
    "        if self.n_classes_ > 2 and self.multi_class == \"one_vs_one\":\n",
    "            raise ValueError(\"one_vs_one multi-class mode does not support \"\n",
    "                             \"predicting probability estimates. Use \"\n",
    "                             \"one_vs_rest mode instead.\")\n",
    "        X = check_array(X)\n",
    "        return self.base_estimator_.predict_proba(X)\n",
    "\n",
    "    def predict_var(self, X):\n",
    "\n",
    "        check_is_fitted(self, [\"classes_\", \"n_classes_\"])\n",
    "        if self.n_classes_ > 2 and self.multi_class == \"one_vs_one\":\n",
    "            raise ValueError(\"one_vs_one multi-class mode does not support \"\n",
    "                             \"predicting probability estimates. Use \"\n",
    "                             \"one_vs_rest mode instead.\")\n",
    "        X = check_array(X)\n",
    "        return self.base_estimator_.predict_var(X)\n",
    "\n",
    "    @property\n",
    "    def kernel_(self):\n",
    "        if self.n_classes_ == 2:\n",
    "            return self.base_estimator_.kernel_\n",
    "        else:\n",
    "            return CompoundKernel(\n",
    "                [estimator.kernel_\n",
    "                 for estimator in self.base_estimator_.estimators_])\n",
    "\n",
    "    def log_marginal_likelihood(self, theta=None, eval_gradient=False):\n",
    "        \"\"\"Returns log-marginal likelihood of theta for training data.\n",
    "        In the case of multi-class classification, the mean log-marginal\n",
    "        likelihood of the one-versus-rest classifiers are returned.\n",
    "        Parameters\n",
    "        ----------\n",
    "        theta : array-like, shape = (n_kernel_params,) or none\n",
    "            Kernel hyperparameters for which the log-marginal likelihood is\n",
    "            evaluated. In the case of multi-class classification, theta may\n",
    "            be the  hyperparameters of the compound kernel or of an individual\n",
    "            kernel. In the latter case, all individual kernel get assigned the\n",
    "            same theta values. If None, the precomputed log_marginal_likelihood\n",
    "            of ``self.kernel_.theta`` is returned.\n",
    "        eval_gradient : bool, default: False\n",
    "            If True, the gradient of the log-marginal likelihood with respect\n",
    "            to the kernel hyperparameters at position theta is returned\n",
    "            additionally. Note that gradient computation is not supported\n",
    "            for non-binary classification. If True, theta must not be None.\n",
    "        Returns\n",
    "        -------\n",
    "        log_likelihood : float\n",
    "            Log-marginal likelihood of theta for training data.\n",
    "        log_likelihood_gradient : array, shape = (n_kernel_params,), optional\n",
    "            Gradient of the log-marginal likelihood with respect to the kernel\n",
    "            hyperparameters at position theta.\n",
    "            Only returned when eval_gradient is True.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, [\"classes_\", \"n_classes_\"])\n",
    "\n",
    "        if theta is None:\n",
    "            if eval_gradient:\n",
    "                raise ValueError(\n",
    "                    \"Gradient can only be evaluated for theta!=None\")\n",
    "            return self.log_marginal_likelihood_value_\n",
    "\n",
    "        theta = np.asarray(theta)\n",
    "        if self.n_classes_ == 2:\n",
    "            return self.base_estimator_.log_marginal_likelihood(\n",
    "                theta, eval_gradient)\n",
    "        else:\n",
    "            if eval_gradient:\n",
    "                raise NotImplementedError(\n",
    "                    \"Gradient of log-marginal-likelihood not implemented for \"\n",
    "                    \"multi-class GPC.\")\n",
    "            estimators = self.base_estimator_.estimators_\n",
    "            n_dims = estimators[0].kernel_.n_dims\n",
    "            if theta.shape[0] == n_dims:  # use same theta for all sub-kernels\n",
    "                return np.mean(\n",
    "                    [estimator.log_marginal_likelihood(theta)\n",
    "                     for i, estimator in enumerate(estimators)])\n",
    "            elif theta.shape[0] == n_dims * self.classes_.shape[0]:\n",
    "                # theta for compound kernel\n",
    "                return np.mean(\n",
    "                    [estimator.log_marginal_likelihood(\n",
    "                        theta[n_dims * i:n_dims * (i + 1)])\n",
    "                     for i, estimator in enumerate(estimators)])\n",
    "            else:\n",
    "                raise ValueError(\"Shape of theta must be either %d or %d. \"\n",
    "                                 \"Obtained theta with shape %d.\"\n",
    "                                 % (n_dims, n_dims * self.classes_.shape[0],\n",
    "                                    theta.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## IWARE MODEL FROM PAWS ##\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m minimize\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "## IWARE MODEL FROM PAWS ##\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "# from iware.gpc import GaussianProcessClassifier\n",
    "\n",
    "from os import getenv\n",
    "from multiprocessing import Pool, get_context\n",
    "from functools import partial\n",
    "from os import getpid\n",
    "\n",
    "# from opencensus.ext.azure import metrics_exporter\n",
    "# from opencensus.stats import aggregation as aggregation_module\n",
    "# from opencensus.stats import measure as measure_module\n",
    "# from opencensus.stats import stats as stats_module\n",
    "# from opencensus.stats import view as view_module\n",
    "# from opencensus.tags import tag_map as tag_map_module\n",
    "# from opencensus.tags import tag_key as tag_key_module\n",
    "# from opencensus.tags import tag_value as tag_value_module\n",
    "\n",
    "# appinsights_key = getenv('APPINSIGHTS_INSTRUMENTATIONKEY', None)\n",
    "\n",
    "POSITIVE_LABEL = 1      # how a positive label is encoded in the data\n",
    "RANDOM_SEED = None        # could be None\n",
    "N_JOBS = 1 # -1 to use max\n",
    "\n",
    "# parameters for bagging classifier\n",
    "NUM_ESTIMATORS = 32 #32 #50\n",
    "MAX_SAMPLES = 0.8\n",
    "MAX_FEATURES = .5\n",
    "\n",
    "# verbose output if == 1\n",
    "VERBOSE = 0\n",
    "\n",
    "MULTIPROCESSING_POOL_SIZE_ENV_VAR='MULTIPROCESSING_POOL_SIZE'\n",
    "\n",
    "###########################################################\n",
    "# utility functions\n",
    "###########################################################\n",
    "# given training and predict sets, normalize data to zero mean, unit variance\n",
    "def normalize_data(train, predict):\n",
    "    scaler = StandardScaler()\n",
    "    # fit only on training data\n",
    "    scaler.fit(train)\n",
    "    # apply normalization to training and test data\n",
    "    train = scaler.transform(train)\n",
    "    predict = scaler.transform(predict)\n",
    "\n",
    "    return train, predict\n",
    "\n",
    "# by maximizing F1 score?\n",
    "def determine_threshold(label, predict_test_pos_probs, num_thresholds=50):\n",
    "    # TODO: previously, used tpr-(1-fpr)\n",
    "    # fpr, tpr, thresholds = metrics.roc_curve(label, predict_test_pos_probs, pos_label=POSITIVE_LABEL)\n",
    "    # or maybe scaled, like 2*tpr - (1-fpr)?\n",
    "\n",
    "    thresholds = np.linspace(0, 1, num_thresholds)\n",
    "    f1         = np.zeros(thresholds.size)\n",
    "    precision  = np.zeros(thresholds.size)\n",
    "    recall     = np.zeros(thresholds.size)\n",
    "    auprc      = np.zeros(thresholds.size)\n",
    "\n",
    "    for i in range(num_thresholds):\n",
    "        predict_labels = predict_test_pos_probs > thresholds[i]\n",
    "        predict_labels = predict_labels.astype(int)\n",
    "\n",
    "        f1[i]        = metrics.f1_score(label, predict_labels)\n",
    "        precision[i] = metrics.precision_score(label, predict_labels, pos_label=POSITIVE_LABEL)\n",
    "        recall[i]    = metrics.recall_score(label, predict_labels, pos_label=POSITIVE_LABEL)\n",
    "\n",
    "        precision_vals, recall_vals, _ = metrics.precision_recall_curve(label, predict_test_pos_probs, pos_label=POSITIVE_LABEL)\n",
    "        auprc[i]     = metrics.auc(recall_vals, precision_vals)\n",
    "\n",
    "        if VERBOSE:\n",
    "            print('threshold: {:.4f} | f1: {:.4f},  precision: {:.4f}, recall: {:.4f}, AUPRC: {:.4f}'.format(thresholds[i], f1[i], precision[i], recall[i], auprc[i]))\n",
    "\n",
    "    # opt = np.argmax(f1)\n",
    "    opt = np.argmax(auprc)\n",
    "    print('optimal threshold {:.4f}, with f1 {:.4f}, precision {:.4f}, recall {:.4f}, AUPRC {:.4f}'.format(thresholds[opt], f1[opt], precision[opt], recall[opt], auprc[opt]))\n",
    "\n",
    "    return thresholds[opt]\n",
    "\n",
    "# get classifier used as base estimator in bagging classifier\n",
    "def get_base_estimator(method):\n",
    "    if method == 'gaussian_processes':\n",
    "        kernel = 1.0 * RBF(length_scale=1.0)\n",
    "        base_estimator = GaussianProcessClassifier(kernel=kernel, random_state=RANDOM_SEED, warm_start=True, max_iter_predict=100, n_jobs=-1)\n",
    "    elif method == 'svm':\n",
    "        base_estimator = SVC(gamma='auto', random_state=RANDOM_SEED)\n",
    "    elif method == 'linear-svc':\n",
    "        base_estimator = LinearSVC(max_iter=5000, random_state=RANDOM_SEED)\n",
    "    elif method == 'decision_tree':\n",
    "        base_estimator = tree.DecisionTreeClassifier(random_state=RANDOM_SEED)\n",
    "    else:\n",
    "        raise Exception('method \\'{}\\' not recognized'.format(method))\n",
    "\n",
    "    return base_estimator\n",
    "\n",
    "\n",
    "# get overall classifier to use\n",
    "def get_classifier(use_balanced, method):\n",
    "    if method == 'random_forest':\n",
    "        return RandomForestClassifier(n_estimators=NUM_ESTIMATORS,\n",
    "            criterion='gini', max_depth=None, min_samples_split=2,\n",
    "            min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "            max_features=MAX_FEATURES, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            bootstrap=True, oob_score=False, n_jobs=N_JOBS,\n",
    "            random_state=RANDOM_SEED, verbose=VERBOSE,\n",
    "            warm_start=False, class_weight=None)\n",
    "\n",
    "    base_estimator = get_base_estimator(method)\n",
    "\n",
    "    if method == 'gaussian_processes':\n",
    "        # gaussian_processess don't need a bagging classifier\n",
    "        return base_estimator\n",
    "    elif use_balanced:\n",
    "        # balanced bagging classifier used for datasets with strong label imbalance\n",
    "        return BalancedBaggingClassifier(base_estimator=base_estimator,\n",
    "            n_estimators=NUM_ESTIMATORS, max_samples=MAX_SAMPLES,\n",
    "            max_features=MAX_FEATURES,\n",
    "            bootstrap=True, bootstrap_features=False,\n",
    "            oob_score=False, warm_start=False,\n",
    "            sampling_strategy='majority', #sampling_strategy=0.8,\n",
    "            replacement=True, n_jobs=N_JOBS,\n",
    "            random_state=RANDOM_SEED, verbose=VERBOSE)\n",
    "    else:\n",
    "        # non-balanced bagging classifier used for other datasets\n",
    "        return BaggingClassifier(base_estimator=base_estimator,\n",
    "            n_estimators=NUM_ESTIMATORS, max_samples=MAX_SAMPLES,\n",
    "            max_features=MAX_FEATURES,\n",
    "            bootstrap=True, bootstrap_features=False,\n",
    "            oob_score=False, warm_start=False, n_jobs=N_JOBS,\n",
    "            random_state=RANDOM_SEED, verbose=VERBOSE)\n",
    "\n",
    "def train_single_classifier(patrol_threshold, use_balanced, method, train_x, train_y, train_effort):\n",
    "    # reduce to size desired\n",
    "    train_y = np.squeeze(train_y)\n",
    "    \n",
    "    # idx = np.where(np.logical_or(train_effort >= patrol_threshold, train_y == POSITIVE_LABEL))[0]\n",
    "\n",
    "    # print('train_effort', train_effort)\n",
    "    # print('patrol_threshold', patrol_threshold)\n",
    "    # print('train_y', train_y)\n",
    "    # print('train_y.shape', train_y.shape)\n",
    "    # print('reshaped train_y', train_y)\n",
    "    # print('reshaped train_y shape', train_y.shape)\n",
    "    # print('POSITIVE_LABEL', POSITIVE_LABEL)\n",
    "\n",
    "    # print('type', type(train_y))\n",
    "\n",
    "    # print(np.where(train_effort >= patrol_threshold)[0].shape)\n",
    "    # print(np.where(train_y == POSITIVE_LABEL)[0].shape)\n",
    "\n",
    "    # print('np.logical or', np.logical_or(train_effort >= patrol_threshold, train_y == POSITIVE_LABEL))\n",
    "    # print('np.logical or shape', np.logical_or(train_effort >= patrol_threshold, train_y == POSITIVE_LABEL).shape)\n",
    "    # print('np.where', np.where(np.logical_or(train_effort >= patrol_threshold, train_y == POSITIVE_LABEL)))\n",
    "    # # print('np.where shape', np.where(np.logical_or(train_effort >= patrol_threshold, train_y == POSITIVE_LABEL)).shape)\n",
    "    # print('np.where[0] shape', np.where(np.logical_or(train_effort >= patrol_threshold, train_y == POSITIVE_LABEL))[0].shape)\n",
    "    # # print('np.where[1] shape', np.where(np.logical_or(train_effort >= patrol_threshold, train_y == POSITIVE_LABEL))[1].shape)\n",
    "\n",
    "\n",
    "    # idx_where_train_effort = np.where(train_effort >= patrol_threshold)[0]\n",
    "    # print('idx_where_train_effort', idx_where_train_effort.shape)\n",
    "\n",
    "    # idx_where_positive = np.where(train_y == POSITIVE_LABEL)[0]\n",
    "    # print('idx_where_positive', idx_where_positive.shape)\n",
    "\n",
    "    # get index of where to cut off points to analyze where either of these expressions holds (positive label or effort is beyond/at threshold)\n",
    "    idx = np.where(np.logical_or(train_effort >= patrol_threshold, train_y == POSITIVE_LABEL))[0]\n",
    "    \n",
    "    # print('idx shape', idx.shape)\n",
    "    # # print('patrol threshold', patrol_threshold)\n",
    "    # print('train effort', train_effort.shape)\n",
    "    # print('num positive train_y', np.sum(train_y))\n",
    "    # print(' **** Training classifier for threshold: ' + str(patrol_threshold))\n",
    "    # print(\"pid: \", getpid())\n",
    "\n",
    "    # if idx has no points for a given training threshold nothing can happen\n",
    "    if idx.size == 0:\n",
    "        print('no training points found for threshold = {}'.format(patrol_threshold))\n",
    "        return None\n",
    "\n",
    "    # filter data for given idx \n",
    "    train_x_filter = train_x[idx, :]\n",
    "    train_y_filter = train_y[idx]\n",
    "\n",
    "    print('filtered data: {}. num positive labels {}. threshold {}'.format(train_x_filter.shape, np.sum(train_y_filter), patrol_threshold))\n",
    "\n",
    "    # filtered labels have no positive hits, threshold not useful to train on.\n",
    "    if np.sum(train_y_filter) == 0:\n",
    "        print('no positive labels in this subset of the training data. skipping threshold {}'.format(patrol_threshold))\n",
    "        return None\n",
    "\n",
    "    # print('threshold {}, num x {}'.format(patrol_threshold, train_x_filter.shape))\n",
    "\n",
    "    # # print('before raveled', train_y_filter.shape)\n",
    "    # train_y_filter = train_y_filter.ravel() # to get rid of DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
    "    # # print('after raveled', train_y_filter.shape)\n",
    "    # print('ravel train_y_filter done')\n",
    "\n",
    "    # print('train x filter', train_x_filter)\n",
    "    # print('train y filter', train_y_filter)\n",
    "\n",
    "    # fit training data, get classifier and fit function according to method using get_classifier fn\n",
    "    classifier = get_classifier(use_balanced, method)\n",
    "    # print('get_classifier done')\n",
    "    # print('train_x_filter shape', train_x_filter.shape)\n",
    "    # print('train_y_filter shape', train_y_filter.shape)\n",
    "    classifier.fit(train_x_filter, train_y_filter)\n",
    "\n",
    "    # print('single classifier done fitting')\n",
    "\n",
    "    # return fitted classifier\n",
    "    return classifier\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# iWare-E class\n",
    "###########################################################\n",
    "class iWare:\n",
    "    def __init__(self, method, num_classifiers, year, task_id):\n",
    "        self.method = method\n",
    "        self.num_classifiers = num_classifiers\n",
    "        self.year = year\n",
    "        self.patrol_thresholds = None\n",
    "        self.classifiers = None\n",
    "        self.weights = None         # weights for classifiers\n",
    "\n",
    "        self.classifier_train_measure = None\n",
    "        self.stats_recorder = None\n",
    "        self.key_patrol_threshold = None\n",
    "        self.classifier_train_tmap = None\n",
    "\n",
    "        # optional settings to monitor stats on runtime performance of classifier (training duration etc.)\n",
    "        # if (appinsights_key):\n",
    "        #     stats = stats_module.stats\n",
    "        #     view_manager = stats.view_manager\n",
    "        #     self.stats_recorder = stats.stats_recorder\n",
    "\n",
    "        #     self.classifier_train_measure = measure_module.MeasureFloat(\"ClassifierTrainDuration\",\n",
    "        #                                                 \"Classifier train duration in seconds\",\n",
    "        #                                                 \"seconds\")\n",
    "\n",
    "        #     key_task_id = tag_key_module.TagKey(\"task_id\")\n",
    "        #     CLASSIFIER_TRAIN_VIEW = view_module.View(\"ClassifierTrainDuration\",\n",
    "        #                                         \"Classifier train duration in seconds\",\n",
    "        #                                         [key_task_id],\n",
    "        #                                         self.classifier_train_measure,\n",
    "        #                                         aggregation_module.LastValueAggregation())\n",
    "\n",
    "        #     self.classifier_train_tmap = tag_map_module.TagMap()\n",
    "        #     self.classifier_train_tmap.insert(key_task_id, tag_value_module.TagValue(task_id))\n",
    "\n",
    "\n",
    "    ###########################################################\n",
    "    # classification\n",
    "    ###########################################################\n",
    "\n",
    "    def get_patrol_thresholds(self, train_effort):\n",
    "        patrol_threshold_percentile = np.linspace(0, 100, self.num_classifiers, endpoint=False)\n",
    "        patrol_thresholds = np.percentile(train_effort, patrol_threshold_percentile)\n",
    "        print('percentiles {}'.format(patrol_threshold_percentile))\n",
    "        print('patrol thresholds {}'.format(patrol_thresholds))\n",
    "        return patrol_thresholds\n",
    "\n",
    "    # currently does not use cross validation? or enable tuning of the V_p hyperparameter?\n",
    "    # currently only does trivial case of identity matrix for the final combined matrix?\n",
    "    def get_vote_matrix(self):\n",
    "        vote_power = np.identity(self.num_classifiers)                           # identity matrix\n",
    "        vote_qual = np.ones((self.num_classifiers, self.num_classifiers))\n",
    "\n",
    "        # create combined vote matrix\n",
    "        vote_combine = np.multiply(vote_power, vote_qual)\n",
    "\n",
    "        # normalize column-wise\n",
    "        vote_combine = vote_combine / vote_combine.sum(1)[:,None]\n",
    "\n",
    "        return vote_combine\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # train a set of classifiers using provided data\n",
    "    def train_classifiers(self, patrol_thresholds, train_x, train_y, train_effort, use_balanced):\n",
    "        classifiers = []\n",
    "        pool_size = getenv(MULTIPROCESSING_POOL_SIZE_ENV_VAR, None)\n",
    "        print('pool size', pool_size)\n",
    "        print('Training ' + str(len(patrol_thresholds)) + ' classifiers with a pool size of ' + str(pool_size))\n",
    "\n",
    "        with get_context(\"spawn\").Pool(processes=int(pool_size)) as pool:\n",
    "            # time process\n",
    "            start_time = time.time()\n",
    "            # get classifiers by training each single classifier\n",
    "            classifiers = pool.map(partial(train_single_classifier, use_balanced=use_balanced, method=self.method, train_x=train_x, train_y=train_y, train_effort=train_effort), patrol_thresholds)\n",
    "            # get analytics: finish timing\n",
    "            # if (appinsights_key):\n",
    "            #     dur = (time.time() - start_time)\n",
    "            #     print('Training duration: ' + str(dur))\n",
    "            #     mmap = self.stats_recorder.new_measurement_map()\n",
    "            #     mmap.measure_float_put(self.classifier_train_measure, dur)\n",
    "            #     mmap.record(self.classifier_train_tmap)\n",
    "        print('all train_single_classifiers done')\n",
    "        # return results\n",
    "        return classifiers\n",
    "\n",
    "        # print('patrol thresholds', patrol_thresholds)\n",
    "        # classifiers = []\n",
    "        # for patrol_threshold in patrol_thresholds:\n",
    "        #     trained_classifier = train_single_classifier(patrol_threshold, use_balanced, self.method, train_x, train_y, train_effort)\n",
    "        #     print('trained classifier', trained_classifier)\n",
    "        #     classifiers.append(trained_classifier)\n",
    "        # print('all train_single_classifiers done')\n",
    "        # return classifiers\n",
    "\n",
    "    # training classifiers within the model\n",
    "    def train_iware(self, all_train_x, all_train_y, all_train_effort, use_balanced=False, nsplits=5):\n",
    "        self.patrol_thresholds = self.get_patrol_thresholds(all_train_effort)\n",
    "\n",
    "        print('shape x', all_train_x.shape)\n",
    "        print('shape y', all_train_y.shape)\n",
    "        print('shape train_effort', all_train_effort.shape)\n",
    "\n",
    "        self.weights = self.get_vote_matrix()\n",
    "\n",
    "        print('-------------------------------------------')\n",
    "        print('training classifiers with all train data')\n",
    "        print('-------------------------------------------')\n",
    "\n",
    "        self.classifiers = self.train_classifiers(self.patrol_thresholds, all_train_x, all_train_y, all_train_effort, use_balanced)\n",
    "        print('done train_iware')\n",
    "\n",
    "    ###########################################################\n",
    "    # iWare-E for predicting future risk\n",
    "    ###########################################################\n",
    "    def train(self, predict_section, predict_section_test, features_raw, features, feature_names,\n",
    "            labels, patrol_effort, section_col, input_static_feats,\n",
    "            test_temp=None, test_precip=None, gaussian_processesp_filename=None):\n",
    "        predict_year = self.year\n",
    "        # ----------------------------------------------\n",
    "        # get training data\n",
    "        # ----------------------------------------------\n",
    "        # use all data before specified (predict_year, predict_section)\n",
    "        train_idx = np.where(np.logical_or(features_raw['year'] < predict_year,\n",
    "            np.logical_and(features_raw['year'] == predict_year, features_raw['section'] < predict_section_test)))[0]\n",
    "\n",
    "        train_x = features[train_idx, :]\n",
    "        train_y = labels[train_idx]\n",
    "        train_patrol_effort = patrol_effort[train_idx]\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # get data to predict on\n",
    "        # ----------------------------------------------\n",
    "        if predict_section == 0:\n",
    "            prev_year = predict_year - 1\n",
    "            num_section = np.max(section_col)\n",
    "            prev_section = num_section\n",
    "        else:\n",
    "            prev_year = predict_year\n",
    "            prev_section = predict_section - 1\n",
    "\n",
    "        print('  test section: year {}, section {}'.format(predict_year, predict_section))\n",
    "        print('  prev section: year {}, section {}'.format(prev_year, prev_section))\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # set up data arrays\n",
    "        # ----------------------------------------------\n",
    "        # get past patrol effort for the test section\n",
    "        prev_section_idx = np.where(np.logical_and(features_raw['year'] == prev_year, features_raw['section'] == prev_section))\n",
    "        past_patrol_effort = patrol_effort[prev_section_idx]\n",
    "\n",
    "        prev_section_spatial_id = features_raw['spatial_id'].values[prev_section_idx]\n",
    "        patrol_effort_df = pd.DataFrame({'spatial_id': prev_section_spatial_id,\n",
    "                                            'past_patrol_effort': past_patrol_effort})\n",
    "\n",
    "        # get all static features\n",
    "        # input_static_featsa = list(input_static_feats.columns)\n",
    "        # patrol_effort_dfa = list(patrol_effort_df.columns)\n",
    "\n",
    "\n",
    "        #input_static_feats.drop(columns=input_static_feats.columns[0], inplace=True)\n",
    "        # create features array and add in past_patrol_effort\n",
    "        predict_x_df = input_static_feats.join(patrol_effort_df.set_index('spatial_id'), on='spatial_id', how='left')\n",
    "\n",
    "        # input_static_featsa = list(predict_x_df.columns)\n",
    "\n",
    "        predict_x_df['past_patrol_effort'].fillna(0, inplace=True)\n",
    "\n",
    "        print(predict_x_df)\n",
    "\n",
    "        # add climate info\n",
    "        if test_temp is not None and test_precip is not None:\n",
    "            predict_x_df['temp']   = test_temp * np.ones(input_static_feats.shape[0])\n",
    "            predict_x_df['precip'] = test_precip * np.ones(input_static_feats.shape[0])\n",
    "\n",
    "        # add gaussian_processes info\n",
    "        if gaussian_processesp_filename is not None:\n",
    "            new_gaussian_processesp = pd.read_csv('../preprocess_consolidate/belum_traponly_combined/1000/output/all_3month/gaussian_processesP_2019_0.csv')\n",
    "            predict_x_df['gaussian_processesp'] = new_gaussian_processesp['2019-0']\n",
    "\n",
    "        # arrange columns to match training data\n",
    "        store_columns = predict_x_df[['spatial_id', 'x', 'y']]\n",
    "        predict_x_df.drop(columns=['spatial_id', 'x', 'y'], inplace=True)\n",
    "\n",
    "        predict_x_df = predict_x_df[feature_names]\n",
    "        predict_x = predict_x_df.values\n",
    "\n",
    "        # normalize data\n",
    "        train_x, predict_x = normalize_data(train_x, predict_x)\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # train classifiers\n",
    "        # ----------------------------------------------\n",
    "        print('training classifiers on {} points...'.format(train_x.shape))\n",
    "\n",
    "        train_start_time = time.time()\n",
    "        self.train_iware(train_x, train_y, train_patrol_effort)\n",
    "        total_train_time = time.time() - train_start_time\n",
    "        print('total train time {:.3f}'.format(total_train_time))\n",
    "        return train_x, predict_x, store_columns\n",
    "\n",
    "    # use all provided data to make predictions\n",
    "    def make_predictions(self, predict_section, train_x, predict_x, store_columns):\n",
    "        # ----------------------------------------------\n",
    "        # run classifiers to get set of predictions\n",
    "        # ----------------------------------------------\n",
    "        # intiialize array to store predictions from each classifier\n",
    "        predict_year = self.year\n",
    "        print('making predictions on year {} section {}... {} points'.format(predict_year, predict_section, predict_x.shape))\n",
    "        final_predictions = np.zeros((predict_x.shape[0], self.num_classifiers))\n",
    "\n",
    "        print('final_predictions', final_predictions)\n",
    "\n",
    "        if self.method == 'gaussian_processes' or self.method == 'random_forest':\n",
    "            final_variances = np.zeros((predict_x.shape[0], self.num_classifiers))\n",
    "\n",
    "        print('self.num_classifiers', self.num_classifiers)\n",
    "\n",
    "        print('0')\n",
    "\n",
    "        # make predictions with each classifier\n",
    "        for i in range(self.num_classifiers):\n",
    "\n",
    "            print('')\n",
    "            print('ITERATION', i)\n",
    "            print('')\n",
    "\n",
    "            # this classifier had no training points, so we skip it\n",
    "            if self.classifiers[i] is None:\n",
    "                final_predictions[:, i] = np.zeros((final_predictions.shape[0]))\n",
    "                continue\n",
    "\n",
    "            print('self.classifiers[i]', self.classifiers[i])\n",
    "\n",
    "            if self.method == 'gaussian_processes' or self.method == 'random_forest':\n",
    "                if self.method == 'random_forest':\n",
    "                    curr_predictions, curr_variances = self.classifiers[i].predict_proba(predict_x, return_var=True, train_x=train_x)\n",
    "                else:\n",
    "                    curr_predictions = self.classifiers[i].predict_proba(predict_x)\n",
    "                    curr_variances = self.classifiers[i].predict_var(predict_x)\n",
    "\n",
    "                print('variance min {} max {}'.format(np.min(curr_variances), np.max(curr_variances)))\n",
    "\n",
    "                final_variances[:, i] = curr_variances\n",
    "            else:\n",
    "                curr_predictions = self.classifiers[i].predict_proba(predict_x)\n",
    "\n",
    "\n",
    "            # curr_predictions has two columns, first column for probability of 0, second col for prob of 1\n",
    "            # if on classifier iter 8 and 9, number of filtered datapoints = number of positive illegal activity\n",
    "            # this is because for higher patrol thresholds, there may not be any datapoints from train_x that have greater patrol effort\n",
    "            # so for the np.where constructing filtered data, the selected data is just the illegal data,\n",
    "            # so we can predict 100% confidence those have illegal instances\n",
    "            # so for iter 8 and 9 it's just 1 column because the prob is all 1 so it'll be e.g. (97,1) rather than (97,2)\n",
    "\n",
    "            # print('curr_predictions', curr_predictions)\n",
    "            print(np.size(curr_predictions))\n",
    "\n",
    "            # for (n,1) case, where all n probabilities are 1\n",
    "            if np.size(curr_predictions) == curr_predictions.shape[0]:\n",
    "                curr_predictions = np.ravel(curr_predictions)\n",
    "            # for (n,2) case, first column for probability of 0, second col for prob of 1\n",
    "            else:\n",
    "                print('in')\n",
    "                curr_predictions = curr_predictions[:, 1]   # probability of positive label\n",
    "\n",
    "            final_predictions[:, i] = curr_predictions\n",
    "\n",
    "            print('final_predictions', final_predictions)\n",
    "\n",
    "        # save out predictions to CSV\n",
    "        print('  save out predictions...')\n",
    "        predictions_df = pd.DataFrame(data=final_predictions, columns=['threshold={}'.format(thresh) for thresh in self.patrol_thresholds])\n",
    "\n",
    "        predictions_df = pd.concat([store_columns, predictions_df], axis=1)\n",
    "\n",
    "        # save out variances to CSV\n",
    "        variances_df = None\n",
    "        if self.method == 'gaussian_processes' or self.method == 'random_forest':\n",
    "            print('  save out variances...')\n",
    "            variances_df = pd.DataFrame(data=final_variances, columns=['threshold={}'.format(thresh) for thresh in self.patrol_thresholds])\n",
    "            variances_df = pd.concat([store_columns, variances_df], axis=1)\n",
    "        return predictions_df, variances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREDICTION FROM PAWS ##\n",
    "# predictions.py: defines functions to run predictions and do data setup within the prediction generation\n",
    "\n",
    "# imports\n",
    "# from iware.iware import * # custom package -> need to determine what to use here\n",
    "# from iware import * # custom package -> need to determine what to use here\n",
    "import calendar # default package\n",
    "import copy # default package\n",
    "\n",
    "# cuts useless stuff from big data frame before prediction\n",
    "def setup_data(features_raw, labels_raw):\n",
    "    # TODO mod to be trapping effort\n",
    "    patrol_effort = features_raw['current_patrol_effort'].values\n",
    "    section_col   = features_raw['section'].values\n",
    "    year_col      = features_raw['year'].values\n",
    "\n",
    "    # drop first column and the current_patrol_effort after grabbing the values and the corresponding first col label\n",
    "    features_raw.drop(columns=features_raw.columns[0], inplace=True)\n",
    "    features_raw.drop(columns='current_patrol_effort', inplace=True)\n",
    "    labels_raw.drop(columns=labels_raw.columns[0], inplace=True)\n",
    "\n",
    "    # get rid of following column labels \n",
    "    labels_raw.drop(columns='global_id', inplace=True)\n",
    "    labels_raw.drop(columns='year', inplace=True)\n",
    "    labels_raw.drop(columns='section', inplace=True)\n",
    "    labels_raw.drop(columns='spatial_id', inplace=True)\n",
    "    labels_raw.drop(columns='x', inplace=True)\n",
    "    labels_raw.drop(columns='y', inplace=True)\n",
    "\n",
    "    # make a copy of the features that removes the removed labels from ebfore\n",
    "    features = copy.copy(features_raw)\n",
    "    features.drop(columns='global_id', inplace=True)\n",
    "    features.drop(columns='year', inplace=True)\n",
    "    features.drop(columns='section', inplace=True)\n",
    "    features.drop(columns='spatial_id', inplace=True)\n",
    "    features.drop(columns='x', inplace=True)\n",
    "    features.drop(columns='y', inplace=True)\n",
    "\n",
    "    # grab the values from both edited objects \n",
    "    features = features.values\n",
    "    labels   = labels_raw.values\n",
    "\n",
    "    # convert column labels into a list to grab the feature names and remove the irrelevant feature names, grab label names\n",
    "    feature_names = list(features_raw.columns)\n",
    "    feature_names.remove('global_id')\n",
    "    feature_names.remove('year')\n",
    "    feature_names.remove('section')\n",
    "    feature_names.remove('spatial_id')\n",
    "    feature_names.remove('x')\n",
    "    feature_names.remove('y')\n",
    "    label_names = list(labels_raw.columns)\n",
    "\n",
    "    # check feature and label names are converted properly\n",
    "    print('feature names {}'.format(feature_names))\n",
    "    print('label names {}'.format(label_names))\n",
    "\n",
    "    # check shapes of all feature objects created are what we expected\n",
    "    print('features_raw: ' + str(np.shape(features_raw)))\n",
    "    print('features: ' + str(np.shape(features)))\n",
    "    print('feature_names: ' + str(np.shape(feature_names)))\n",
    "    # return necessary data\n",
    "    return features_raw, features, feature_names, labels, patrol_effort, section_col, year_col\n",
    "\n",
    "\n",
    "\n",
    "# making predictions runner function\n",
    "# models accepted: decision_tree, random_forest, gaussian_processes\n",
    "def run_predictions(TRAIN_SECTION, log, taskId,\n",
    "                    dataIO_helper,\n",
    "                    months_prefix,\n",
    "                    temporal_training_resolution_month_count,\n",
    "                    classifier_model,\n",
    "                    prediction_start_year,\n",
    "                    container_name,\n",
    "                    run_id):\n",
    "    # find number of sections\n",
    "    section_count = int(12 / temporal_training_resolution_month_count)\n",
    "    \n",
    "    # gather the data into its final format\n",
    "    get_top_directory = 'post_processed_data'\n",
    "    path = '{}_{}month'.format(months_prefix, temporal_training_resolution_month_count)\n",
    "    x_filename = '{}_x.csv'.format(months_prefix)\n",
    "    features_raw = dataIO_helper.get_csv(x_filename, get_top_directory, path)\n",
    "\n",
    "    path = '{}_{}month'.format(months_prefix, temporal_training_resolution_month_count)\n",
    "    y_filename = '{}_y.csv'.format(months_prefix)\n",
    "    labels_raw = dataIO_helper.get_csv(y_filename, get_top_directory, path)\n",
    "\n",
    "    log.log_info('predict: {}'.format('Running setup_data.'), taskId=taskId)\n",
    "    features_raw, features, feature_names, labels, patrol_effort, section_col, year_col = setup_data(features_raw, labels_raw)\n",
    "    # get the classifier count \n",
    "    classifier_count = getenv('NUM_CLASSIFIERS')\n",
    "    classifier_count = int(classifier_count)\n",
    "\n",
    "    # create a model instance\n",
    "    iware_model = iWare(classifier_model, classifier_count, prediction_start_year, taskId)\n",
    "\n",
    "    # Train model #\n",
    "    # get static features\n",
    "    geo_features_csv = dataIO_helper.get_csv('static_features.csv', get_top_directory)\n",
    "\n",
    "    # train the model\n",
    "    train_x, predict_x, store_columns = iware_model.train(TRAIN_SECTION, int(getenv('TEST_SECTION')), features_raw, features, feature_names, labels, patrol_effort, section_col, geo_features_csv)\n",
    "\n",
    "    # iterate over number of sections\n",
    "    for section in range(1, section_count + 1):\n",
    "        # find month range\n",
    "        month_end_num = temporal_training_resolution_month_count * section\n",
    "        month_end = calendar.month_name[month_end_num]\n",
    "        month_start_num = month_end_num - (temporal_training_resolution_month_count - 1)\n",
    "        month_start = calendar.month_name[month_start_num]\n",
    "\n",
    "        # log and make predictions for given month range\n",
    "        log.log_info('predict: {}'.format('Making predictions for section: {} - {}-{}'.format(str(section), month_start, month_end)), taskId=taskId)\n",
    "        print('Making predictions for section: {} - {}-{}'.format(str(section), month_start, month_end))\n",
    "\n",
    "        predictions, variances = iware_model.make_predictions(section, train_x, predict_x, store_columns)\n",
    "\n",
    "        # add predictions to predictions directory\n",
    "        write_dir = 'risk_prediction'\n",
    "        if section_count == 1:\n",
    "            dataIO_helper.write_csv(predictions, 'predictions_{}.csv'.format(prediction_start_year), write_dir)\n",
    "        else:\n",
    "            dataIO_helper.write_csv(predictions, 'predictions_{}-{}_{}.csv'.format(month_start, month_end, prediction_start_year), write_dir)\n",
    "\n",
    "        # add variances of those predictions to the write directory if desired\n",
    "        if variances:\n",
    "            variances_csv_str = variances.to_csv(index=False, encoding='utf-8')\n",
    "            if section_count == 1:\n",
    "                dataIO_helper.write_csv(variances_csv_str, 'variances_{}.csv'.format(prediction_start_year), write_dir)\n",
    "            else:\n",
    "                month_end = calendar.month_name[temporal_training_resolution_month_count * section]\n",
    "                month_start = calendar.month_name[month_end - temporal_training_resolution_month_count + 1]\n",
    "                dataIO_helper.write_csv(variances_csv_str, 'variances_{}-{}_{}.csv'.format(month_start, month_end, prediction_start_year), write_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
