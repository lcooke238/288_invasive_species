{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "import synthetic_data\n",
    "import tensorflow as tf\n",
    "\n",
    "class TrapEnvironment:\n",
    "    def __init__(self, num_traps=20):\n",
    "        # Initialize environment parameters\n",
    "        self.num_traps = num_traps\n",
    "        self.n = 25\n",
    "        self.m = 25\n",
    "        self.predator_density = None\n",
    "        self.prey_density = None\n",
    "        self.trap_replacement_rate = 10\n",
    "        self.pts_per_sec = 100\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment to its initial state (just predator and prey)\n",
    "        # Generate initial predator and prey densities\n",
    "        # default params for initial generation num_traj=200,len_traj=50, pts_per_sec=100, save_loc='../Data/val.npy', prey_range=(1, 5), predator_range=(1, 3)\n",
    "        data_init = synthetic_data.generate()\n",
    "        self.prey_density, self.predator_density = data_init[:, :, 0, :], data_init[:, :, 1, :]\n",
    "        return self.predator_density, self.prey_density\n",
    "\n",
    "    def step(self, action):\n",
    "        # Take action (place traps) and observe the next state and reward\n",
    "        # Update predator and prey densities based on the action\n",
    "        # Calculate reward based on the change in predator density\n",
    "        # Return next state, reward, and done flag\n",
    "\n",
    "        # Simulate predator dynamics with traps placed at specified locations\n",
    "        # Here, action is a list of trap locations [(i1, j1), (i2, j2), ..., (in, jn)]\n",
    "        y0 = np.zeros((self.n, self.m, 2))\n",
    "        for i, j in action:\n",
    "            y0[i,j,1] = 10  # place those traps at each cell\n",
    "        y0 = y0.flatten()\n",
    "\n",
    "        # get impact on predator and prey spread after placement window steps\n",
    "        master_sol = np.ndarray((self.n*self.m*2,self.pts_per_sec))\n",
    "        \n",
    "        for _ in range(self.trap_replacement_rate):\n",
    "            # trap solver, only grab single timestep\n",
    "            sol = solve_ivp(synthetic_data.spatial_dynamics_traps, y0=y0, t_span=[0,1], t_eval=np.linspace(0, 1, self.pts_per_sec), args=(self.n, self.m))\n",
    "            # prey solver y0 creation\n",
    "            y_prey = np.zeros((self.n, self.m, 2))\n",
    "            last_dim = int(self.pts_per_sec)\n",
    "            sol_use = sol.y.reshape((self.n, self.m, 2, last_dim))\n",
    "            pred_data_new, trap_data_new = sol_use[:, :, 0, :], sol_use[:, :, 1, :]\n",
    "            # set prey from timestep of interest as predator in y_prey\n",
    "            y_prey[:,:,1] = pred_data_new[:, :, -1]\n",
    "            # grab predator information from prey data\n",
    "            y_prey[:,:,0] = prey_data\n",
    "            y_prey = y_prey.flatten()\n",
    "            # prey solver, only grab single timestep\n",
    "            sol_prey = solve_ivp(synthetic_data.spatial_dynamics, y0=y_prey, t_span=[0,1], t_eval=np.linspace(0, 1, self.pts_per_sec), args=(self.n, self.m))\n",
    "            # create y0 for next run of trap solver, overwrite y0 and prey_data\n",
    "            y0 = np.zeros((self.n, self.m, 2))\n",
    "            sol_prey_use = sol_prey.y.reshape((self.n, self.m, 2, last_dim))\n",
    "            prey_data, predator_data = sol_prey_use[:, :, 0, :], sol_prey_use[:, :, 1, :]\n",
    "            y0[:,:,0] = predator_data[:, :, -1]\n",
    "            prey_data = prey_data[:, :, -1]\n",
    "            # initialize trap locations based on number of desired traps and density, re initialize per replacement time\n",
    "            y0[:,:,1] = trap_data_new[:,:,-1]\n",
    "            y0 = y0.flatten()\n",
    "            master_sol = np.concatenate((master_sol, sol_prey.y), 1)\n",
    "\n",
    "        master_sol = master_sol[:,100:].reshape(self.n, self.m,2,self.pts_per_sec*self.trap_replacement_rate)\n",
    "        \n",
    "        #Extract the predator and prey densities from the solution\n",
    "        self.prey_density , self.predator_density = master_sol[:, :, 0, :], master_sol[:, :, 1, :]\n",
    "\n",
    "        # Calculate reward based on the change in predator density\n",
    "        reward = -np.sum(self.predator_density)\n",
    "        \n",
    "        # Return next state, reward, and done flag (assuming no termination condition for now)\n",
    "        return self.predator_density, self.prey_density, reward, False\n",
    "\n",
    "#define DQN\n",
    "class QNetwork(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(num_actions)\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.dense1(state)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Simple Q-learning algorithm with experience replay\n",
    "class QLearningAgent:\n",
    "    def __init__(self, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.q_network = QNetwork(num_actions)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        self.memory = []\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # Epsilon-greedy policy\n",
    "        if np.random.rand() < 0.1:\n",
    "            return np.random.randint(self.num_actions)  # Explore\n",
    "        else:\n",
    "            q_values = self.q_network(np.array([state], dtype=np.float32))\n",
    "            return np.argmax(q_values[0].numpy())  # Exploit\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def experience_replay(self, batch_size=32):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = np.random.choice(len(self.memory), batch_size, replace=False)\n",
    "        states, targets = [], []\n",
    "        for idx in minibatch:\n",
    "            state, action, reward, next_state, done = self.memory[idx]\n",
    "            states.append(state)\n",
    "            q_values = self.q_network(np.array([state], dtype=np.float32)).numpy()[0]\n",
    "            if done:\n",
    "                q_values[action] = reward\n",
    "            else:\n",
    "                next_q_values = self.q_network(np.array([next_state], dtype=np.float32)).numpy()[0]\n",
    "                q_values[action] = reward + 0.9 * np.max(next_q_values)\n",
    "            targets.append(q_values)\n",
    "        \n",
    "        states = np.array(states, dtype=np.float32)\n",
    "        targets = np.array(targets, dtype=np.float32)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.q_network(states)\n",
    "            loss = tf.reduce_mean(tf.square(targets - q_values))\n",
    "        \n",
    "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
    "\n",
    "# Training loop\n",
    "def train_agent(env, agent, num_episodes=1000):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, _, reward, done = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.experience_replay()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "\n",
    "# Create environment and agent\n",
    "env = TrapEnvironment()\n",
    "agent = QLearningAgent(num_actions=env.n * env.m)\n",
    "\n",
    "# Train the agent\n",
    "train_agent(env, agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
