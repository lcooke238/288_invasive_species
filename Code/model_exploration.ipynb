{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Exploration\n",
    "In this file, I will be investigating the PAWS model for Stackelberg Security Games and Wildlife Conservation: https://projects.iq.harvard.edu/files/teamcore/files/project_paws_publication_01_teamcore_yang_et_al_2014.pdf. In this paper, the authors design a stackelberg game where the two agents are the rangers and the poachers. They play a turn based game where the rangers first setup a patrol strategy. The poachers can then observe this strategy and come up with a poaching strategy in return. The patrol/poaching areas are discritized by splitting the map into a grid, and a strategy is defined by assigning the limited number of rangers to a cell of the grid. From here, utility is assigned to the different outcomes of people present combined with if there are endangered animals present, with positive utility given to catching and stopping poachers and negative utility to poachers without a ranger. Using this score of the board, the rangers then update their strategy to reflect the observed poacher behavior. This cycle repeats with the goal of rangers encountering more poachers.\n",
    "\n",
    "For our approach, we want to use this same model with the agents being the trappers and the invasive species we are looking to capture. To make this modification, we first need to change the utility structure such that we reward traps capturing the invasive species and disincentivize traps being placed somewhere where they dont catch an invasive animal. To do this, we first need to understand how the original paws implementation works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAWS Structural Outline and the modifications we will need to make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From my 91r work junior fall, I have put together a rough outline of the PAWS codebase as it works as a flask app in python. To run the model, researchers do the following things:\n",
    "\n",
    "1. **Setup the Flask App and log**: Sets up the overhead and logging needed to track the execution of the flask app containing this software. This portion also establishes connection to an Azure server containing the data needed to operate the program. This portion, relevant for deployment and scaling, will not be relevant to our implementation.\n",
    "2. **Data Input and Breakdown**: In the PAWS design, researchers take data keys from a JSON file that then allow them to access the data of interest from the cloud. This data comes in 3 parts:\n",
    "    - shapes, or the shapefiles representing the patrolled park (i.e. park boundaries or locations of different features of the landscape like rivers and roads)\n",
    "    - rasters, or the raster files providing supplemental information about a patrolled park (i.e. elevation of different areas in a park, land cover)\n",
    "    - patrol_observations, or a csv containing information about the different patrols used for prediction.\n",
    "    \n",
    "    I have a data dictionary for both the JSON file and patrol_observations file to understand what data goes where. Generally though, for our implementation we will be cutting the JSON mapping to the cloud and grabbing the right datasets and instead creating a function to setup the data we need locally.\n",
    "\n",
    "3. **Data Validation and Error Handling**: this section allows us to verify that necessary features of a given piece of data holds before attempting to process the data for prediction. Problems are broken down into Warnings, Errors, and FatalErrors that will log and kill the program if fatal. Understanding and updating these checks based on the modified reward and data will be important as we test and develop our modifications.\n",
    "4. **Preprocess**: This does all of the data pre-processing, where we grab the boundary, clean the data, get shapefiles, distances, and rasters, calculate effort, and compute illegal activity. Some data verification does happen in this file, so checking if this data verification can be refactored into the data validation section could be useful. For our implementation, I think the preprocessing will look quite similar with the exception of how to process the reward-defining activity.\n",
    "5. **Consolidate**: This consolidates all of the data pieces into a single dataframe for the model to use. The PAWS implementaton of consolidate contains some repeated processes that already exist in preprocess, so for our implementation it will be nice to remove those repeat processes. Besides this, consolidation should look extremely similar for our implementation. Additionally, it may be worth working with everything in a more final format from the beginning to avoid unnecessary overhead, but this would be future work after an initial implementation.\n",
    "6. **Predict**: Here, taking this single dataframe, we ultimately predict the locations for our next round of patrols. For our implementation, the prediction should be identical, so long as the modified reward is captured within the data. If not, then the actual prediction schematic may need to update its scoring to reflect rewarding the successful traps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invasive Species Modifications\n",
    "\n",
    "Working on this, I want to start with the initial framework of the prediction to better understand what it uses and therefore what it needs for modifications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## log handling ##\n",
    "\n",
    "class Log:\n",
    "    def __init__(self, filename=\"../Logs/log.txt\"):\n",
    "        self.filename = filename\n",
    "        # Clear the file's contents if it exists\n",
    "        with open(self.filename, \"w\") as file:\n",
    "            pass\n",
    "\n",
    "    def write_line(self, line):\n",
    "        with open(self.filename, \"a\") as file:\n",
    "            file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data input and breakdown ##\n",
    "\n",
    "def traps_get_data(log):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data validation and error handling ##\n",
    "\n",
    "def traps_validate_data(log):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess ##\n",
    "\n",
    "def traps_preprocess(log):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## consolidate ##\n",
    "\n",
    "def traps_consolidate(log):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict ##\n",
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from os import getenv, path\n",
    "from functools import partial\n",
    "from multiprocessing import get_context\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from gpc import GaussianProcessClassifier\n",
    "\n",
    "import copy\n",
    "\n",
    "# global vars\n",
    "MULTIPROCESSING_POOL_SIZE_ENV_VAR='MULTIPROCESSING_POOL_SIZE'\n",
    "POSITIVE_LABEL = 1      # how a positive label is encoded in the data\n",
    "RANDOM_SEED = None        # could be None\n",
    "N_JOBS = 1 # -1 to use max\n",
    "# parameters for bagging classifier\n",
    "NUM_ESTIMATORS = 32 #32 #50\n",
    "MAX_SAMPLES = 0.8\n",
    "MAX_FEATURES = .5\n",
    "# verbose output if == 1\n",
    "VERBOSE = 0\n",
    "\n",
    "TRAIN_SECTION = 1\n",
    "\n",
    "\n",
    "# define model class to train and predict from\n",
    "class SSGModel:\n",
    "    # setup params for the class\n",
    "    def __init__(self, classifier_model, classifier_count, pred_start_timestep, log):\n",
    "        self.classifier_model = classifier_model\n",
    "        self.classifier_count = classifier_count\n",
    "        self.pred_start_timestep = pred_start_timestep\n",
    "        self.log = log\n",
    "        self.trap_thresholds = None\n",
    "        self.classifiers = None\n",
    "        self.weights = None         # weights for classifiers\n",
    "\n",
    "\n",
    "    # training classifiers based on the environment\n",
    "    def train_single_classifier(self, patrol_threshold, use_balanced, train_x, train_y, train_effort):\n",
    "        # reduce to size desired\n",
    "        train_y = np.squeeze(train_y)\n",
    "        \n",
    "        # get index of where to cut off points to analyze where either of these expressions holds (positive label or effort is beyond/at threshold)\n",
    "        idx = np.where(np.logical_or(train_effort >= patrol_threshold, train_y == POSITIVE_LABEL))[0]\n",
    "\n",
    "        # if idx has no points for a given training threshold nothing can happen\n",
    "        if idx.size == 0:\n",
    "            print('no training points found for threshold = {}'.format(patrol_threshold))\n",
    "            return None\n",
    "\n",
    "        # filter data for given idx \n",
    "        train_x_filter = train_x[idx, :]\n",
    "        train_y_filter = train_y[idx]\n",
    "\n",
    "        print('filtered data: {}. num positive labels {}. threshold {}'.format(train_x_filter.shape, np.sum(train_y_filter), patrol_threshold))\n",
    "\n",
    "        # filtered labels have no positive hits, threshold not useful to train on.\n",
    "        if np.sum(train_y_filter) == 0:\n",
    "            print('no positive labels in this subset of the training data. skipping threshold {}'.format(patrol_threshold))\n",
    "            return None\n",
    "\n",
    "        # fit training data, get classifier and fit function according to method using get_classifier fn\n",
    "        # TODO Fill in to pick classifier by method\n",
    "        # classifier = get_classifier(use_balanced, method)\n",
    "        \n",
    "        if self.method == 'random_forest':\n",
    "            classifier = RandomForestClassifier(n_estimators=NUM_ESTIMATORS,\n",
    "                    criterion='gini', max_depth=None, min_samples_split=2,\n",
    "                    min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "                    max_features=MAX_FEATURES, max_leaf_nodes=None,\n",
    "                    min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                    bootstrap=True, oob_score=False, n_jobs=N_JOBS,\n",
    "                    random_state=RANDOM_SEED, verbose=VERBOSE,\n",
    "                    warm_start=False, class_weight=None)\n",
    "\n",
    "        else:\n",
    "            if self.method == 'gaussian_processes':\n",
    "                kernel = 1.0 * RBF(length_scale=1.0)\n",
    "                base_estimator = GaussianProcessClassifier(kernel=kernel, random_state=RANDOM_SEED, warm_start=True, max_iter_predict=100, n_jobs=-1)\n",
    "            elif self.method == 'svm':\n",
    "                base_estimator = SVC(gamma='auto', random_state=RANDOM_SEED)\n",
    "            elif self.method == 'linear-svc':\n",
    "                base_estimator = LinearSVC(max_iter=5000, random_state=RANDOM_SEED)\n",
    "            elif self.method == 'decision_tree':\n",
    "                base_estimator = tree.DecisionTreeClassifier(random_state=RANDOM_SEED)\n",
    "            else:\n",
    "                raise Exception('method \\'{}\\' not recognized'.format(self.method))\n",
    "\n",
    "            if self.method == 'gaussian_processes':\n",
    "                classifier = base_estimator\n",
    "            \n",
    "            elif use_balanced:\n",
    "                # balanced bagging classifier used for datasets with strong label imbalance\n",
    "                classifier = BalancedBaggingClassifier(base_estimator=base_estimator,\n",
    "                    n_estimators=NUM_ESTIMATORS, max_samples=MAX_SAMPLES,\n",
    "                    max_features=MAX_FEATURES,\n",
    "                    bootstrap=True, bootstrap_features=False,\n",
    "                    oob_score=False, warm_start=False,\n",
    "                    sampling_strategy='majority', #sampling_strategy=0.8,\n",
    "                    replacement=True, n_jobs=N_JOBS,\n",
    "                    random_state=RANDOM_SEED, verbose=VERBOSE)\n",
    "            else:\n",
    "                # non-balanced bagging classifier used for other datasets\n",
    "                classifier = BaggingClassifier(base_estimator=base_estimator,\n",
    "                        n_estimators=NUM_ESTIMATORS, max_samples=MAX_SAMPLES,\n",
    "                        max_features=MAX_FEATURES,\n",
    "                        bootstrap=True, bootstrap_features=False,\n",
    "                        oob_score=False, warm_start=False, n_jobs=N_JOBS,\n",
    "                        random_state=RANDOM_SEED, verbose=VERBOSE)\n",
    "        \n",
    "        classifier.fit(train_x_filter, train_y_filter)\n",
    "\n",
    "        # return fitted classifier\n",
    "        return classifier\n",
    "            \n",
    "        \n",
    "    # define full training\n",
    "    def train(self, predict_section, predict_section_test, features_raw, features, feature_names, labels, trap_effort, section_col, input_static_feats, test_temp=None, test_precip=None,use_balanced=False):\n",
    "        \n",
    "        predict_timestep = self.pred_start_timestep\n",
    "\n",
    "        # split data for training and predicting: use all data before specified (predict_year, predict_section)\n",
    "        train_idx = np.where(np.logical_or(features_raw['timestep'] < predict_timestep,\n",
    "            np.logical_and(features_raw['timestep'] == predict_timestep, features_raw['section'] < predict_section_test)))[0]\n",
    "\n",
    "        train_x = features[train_idx, :]\n",
    "        train_y = labels[train_idx]\n",
    "        train_trap_effort = trap_effort[train_idx]\n",
    "\n",
    "        if predict_section == 0:\n",
    "            prev_timestep = predict_timestep - 1\n",
    "            num_section = np.max(section_col)\n",
    "            prev_section = num_section\n",
    "        else:\n",
    "            prev_timestep = predict_timestep\n",
    "            prev_section = predict_section - 1\n",
    "\n",
    "        self.log.write_line(\"training/predicting data split complete. Test at timestep {} in section {}\".format( predict_timestep, predict_section))\n",
    "\n",
    "        # set up data arrays #\n",
    "        # get past trapping effort for the test section\n",
    "        prev_section_idx = np.where(np.logical_and(features_raw['timestep'] == prev_timestep, features_raw['section'] == prev_section))\n",
    "        past_trap_effort = trap_effort[prev_section_idx]\n",
    "\n",
    "        prev_section_spatial_id = features_raw['spatial_id'].values[prev_section_idx]\n",
    "        trap_effort_df = pd.DataFrame({'spatial_id': prev_section_spatial_id,\n",
    "                                            'past_trap_effort': past_trap_effort})\n",
    "\n",
    "        # create features array and add in past_patrol_effort\n",
    "        # TODO uses spatial_id to join, will dataframe we use still have this?\n",
    "        predict_x_df = input_static_feats.join(trap_effort_df.set_index('spatial_id'), on='spatial_id', how='left')\n",
    "        predict_x_df['past_patrol_effort'].fillna(0, inplace=True)\n",
    "\n",
    "        # add climate info\n",
    "        # TODO determine if climate info will still be used\n",
    "        if test_temp is not None and test_precip is not None:\n",
    "            predict_x_df['temp']   = test_temp * np.ones(input_static_feats.shape[0])\n",
    "            predict_x_df['precip'] = test_precip * np.ones(input_static_feats.shape[0])\n",
    "\n",
    "        # arrange columns to match training data\n",
    "        store_columns = predict_x_df[['spatial_id', 'x', 'y']]\n",
    "        predict_x_df.drop(columns=['spatial_id', 'x', 'y'], inplace=True)\n",
    "        predict_x_df = predict_x_df[feature_names]\n",
    "        predict_x = predict_x_df.values\n",
    "        \n",
    "        # normalize data\n",
    "        scaler = StandardScaler()\n",
    "        # fit only on training data\n",
    "        scaler.fit(train_x)\n",
    "        # apply normalization to training and test data\n",
    "        train_x = scaler.transform(train_x)\n",
    "        predict_x = scaler.transform(predict_x)\n",
    "\n",
    "        # train classifiers #\n",
    "        self.log.write_line(\"training classifiers on {} points...\".format(train_x.shape)) # iware\n",
    "        train_start_time = time.time()\n",
    "        self.trap_thresholds = self.get_patrol_thresholds(train_trap_effort)\n",
    "        vote_power = np.identity(self.num_classifiers)                           #get_vote_matrix: identity matrix\n",
    "        vote_qual = np.ones((self.num_classifiers, self.num_classifiers))\n",
    "        # create combined vote matrix\n",
    "        self.weights = np.multiply(vote_power, vote_qual)\n",
    "        # normalize column-wise\n",
    "        self.weights =  self.weights /  self.weights.sum(1)[:,None]\n",
    "        self.log.write_line(\"finished calculating weights.\")\n",
    "        #train classifiers\n",
    "        # self.classifiers = self.train_classifiers(self.patrol_thresholds, train_x, train_y, train_trap_effort, use_balanced)\n",
    "        self.log.write_line(\"begin setup multi-classifier training.\")\n",
    "        self.classifiers = []\n",
    "        pool_size = getenv(MULTIPROCESSING_POOL_SIZE_ENV_VAR, None)\n",
    "        self.log.write_line('Training ' + str(len(self.trap_thresholds)) + ' classifiers with a pool size of ' + str(pool_size))\n",
    "        with get_context(\"spawn\").Pool(processes=int(pool_size)) as pool:\n",
    "            # time process\n",
    "            start_time = time.time()\n",
    "            # get classifiers by training each single classifier\n",
    "            self.classifiers = pool.map(partial(self.train_single_classifier, use_balanced, train_x=train_x, train_y=train_y, train_effort=train_trap_effort), self.trap_thresholds)\n",
    "            self.log.write_line(\"done training all classifiers.\")\n",
    "        # keep individual classifier training abstracted to support different models\n",
    "        total_train_time = time.time() - train_start_time\n",
    "        self.log.write_line(\"done training classifiers. total train time {:.3f}\".format(total_train_time))\n",
    "        \n",
    "        return train_x, predict_x, store_columns\n",
    "\n",
    "    \n",
    "    #define predicting\n",
    "    def predict(self, predict_section, train_x, predict_x, store_columns):\n",
    "        # def make_predictions(self, predict_section, train_x, predict_x, store_columns):\n",
    "        # ----------------------------------------------\n",
    "        # run classifiers to get set of predictions\n",
    "        # ----------------------------------------------\n",
    "        # intiialize array to store predictions from each classifier\n",
    "        predict_year = self.pred_start_timestep\n",
    "        self.log.write_line('making predictions on year {} section {}... {} points'.format(predict_year, predict_section, predict_x.shape))\n",
    "        final_predictions = np.zeros((predict_x.shape[0], self.num_classifiers))\n",
    "        if self.method == 'gaussian_processes' or self.method == 'random_forest':\n",
    "            final_variances = np.zeros((predict_x.shape[0], self.num_classifiers))\n",
    "\n",
    "\n",
    "        # make predictions with each classifier\n",
    "        for i in range(self.num_classifiers):\n",
    "\n",
    "            self.log.write_line('iteration ' + str(i))\n",
    "\n",
    "            # this classifier had no training points, so we skip it\n",
    "            if self.classifiers[i] is None:\n",
    "                final_predictions[:, i] = np.zeros((final_predictions.shape[0]))\n",
    "                continue\n",
    "\n",
    "            if self.method == 'gaussian_processes' or self.method == 'random_forest':\n",
    "                if self.method == 'random_forest':\n",
    "                    curr_predictions, curr_variances = self.classifiers[i].predict_proba(predict_x, return_var=True, train_x=train_x)\n",
    "                else:\n",
    "                    curr_predictions = self.classifiers[i].predict_proba(predict_x)\n",
    "                    curr_variances = self.classifiers[i].predict_var(predict_x)\n",
    "\n",
    "                print('variance min {} max {}'.format(np.min(curr_variances), np.max(curr_variances)))\n",
    "\n",
    "                final_variances[:, i] = curr_variances\n",
    "            else:\n",
    "                curr_predictions = self.classifiers[i].predict_proba(predict_x)\n",
    "\n",
    "            # curr_predictions has two columns, first column for probability of 0, second col for prob of 1\n",
    "            # if on classifier iter 8 and 9, number of filtered datapoints = number of positive illegal activity\n",
    "            # this is because for higher patrol thresholds, there may not be any datapoints from train_x that have greater patrol effort\n",
    "            # so for the np.where constructing filtered data, the selected data is just the illegal data,\n",
    "            # so we can predict 100% confidence those have illegal instances\n",
    "            # so for iter 8 and 9 it's just 1 column because the prob is all 1 so it'll be e.g. (97,1) rather than (97,2)\n",
    "\n",
    "            # for (n,1) case, where all n probabilities are 1\n",
    "            if np.size(curr_predictions) == curr_predictions.shape[0]:\n",
    "                curr_predictions = np.ravel(curr_predictions)\n",
    "            # for (n,2) case, first column for probability of 0, second col for prob of 1\n",
    "            else:\n",
    "                print('in')\n",
    "                curr_predictions = curr_predictions[:, 1]   # probability of positive label\n",
    "\n",
    "            final_predictions[:, i] = curr_predictions\n",
    "\n",
    "\n",
    "        self.log.write_line(\"finished making predictions. Now saving to csv...\")\n",
    "\n",
    "        # save out predictions to CSV\n",
    "        predictions_df = pd.DataFrame(data=final_predictions, columns=['threshold={}'.format(thresh) for thresh in self.patrol_thresholds])\n",
    "        predictions_df = pd.concat([store_columns, predictions_df], axis=1)\n",
    "\n",
    "        # save out variances to CSV\n",
    "        variances_df = None\n",
    "        if self.method == 'gaussian_processes' or self.method == 'random_forest':\n",
    "            self.log.write_line(\"Now saving variances csv...\")\n",
    "            variances_df = pd.DataFrame(data=final_variances, columns=['threshold={}'.format(thresh) for thresh in self.patrol_thresholds])\n",
    "            variances_df = pd.concat([store_columns, variances_df], axis=1)\n",
    "        self.log.write_line(\"returning predictions.\")\n",
    "        return predictions_df, variances_df\n",
    "\n",
    "\n",
    "# TODO: requires data input as 2 separate csvs:\n",
    "    # one of the features for the given region of predictive interest \n",
    "    # one of the labels for the given region of predictive interest\n",
    "# inputs to this function:\n",
    "    # log: instance of the log class. Allows for logging information about a run to a specified text file\n",
    "    # features_file_path: relative path from this script to the features csv file\n",
    "    # labels_file_path: relative path from this script to the labels csv file\n",
    "    # static_features_path: relative path from this script to the static_features csv file\n",
    "    # classifier_model: name of the type of classifier I want to use\n",
    "    # pred_start_timestep: int value of timestep in input data to split training and predicting data\n",
    "    # write_dir: directory to write predictions to\n",
    "def traps_predict(log, features_file_path, labels_file_path, static_features_path, classifier_model, pred_start_timestep, write_dir):\n",
    "    # find number of sections (here split by month) --> TODO may get rid of\n",
    "    # section_count = int(12 / temporal_training_resolution_month_count)\n",
    "    \n",
    "    # gather the data into its final format as a pd dataframe:\n",
    "    # get post-processed features csv (raw features)\n",
    "    features_raw = pd.read_csv(features_file_path)\n",
    "    # get post-processed labels (raw labels)\n",
    "    labels_raw = pd.read_csv(labels_file_path)\n",
    "    # get static features\n",
    "    geo_features_csv = pd.read_csv(static_features_path)\n",
    "    log.write_line(\"final data read in successfully.\")\n",
    "\n",
    "    # generate the following:\n",
    "        # patrol_effort: features_raw['current_patrol_effort'].values, direct value from raw features csv\n",
    "        # section_col: features_raw['section'].values, direct value from raw features csv\n",
    "        # features_raw: overwrite pulled raw features csv that cuts the current patrol effort and the first column (TODO what is that first column, perhaps empty overhead?)\n",
    "        # features: copy of overwritten features_raw that removes global_id, year, section, spatial_id, x, y and only contains the values of the remaining content\n",
    "        # feature_names: object that contains the names of all the remaining values captured in features\n",
    "        # labels: modified original labels that removes the first column, global_id, year, section, spatial_id, x, y, and only contains the values of the remaining content\n",
    "    # TODO mod to be trapping effort\n",
    "    trap_effort = features_raw['current_trapping_effort'].values\n",
    "    section_col   = features_raw['section'].values\n",
    "    # year_col      = features_raw['timestep'].values\n",
    "    # drop first column and the current_patrol_effort after grabbing the values and the corresponding first col label\n",
    "    features_raw.drop(columns=features_raw.columns[0], inplace=True)\n",
    "    features_raw.drop(columns='current_trapping_effort', inplace=True)\n",
    "    labels_raw.drop(columns=labels_raw.columns[0], inplace=True)\n",
    "    # get rid of following column labels \n",
    "    labels_raw.drop(columns='global_id', inplace=True)\n",
    "    labels_raw.drop(columns='timestep', inplace=True)\n",
    "    labels_raw.drop(columns='section', inplace=True)\n",
    "    labels_raw.drop(columns='spatial_id', inplace=True)\n",
    "    labels_raw.drop(columns='x', inplace=True)\n",
    "    labels_raw.drop(columns='y', inplace=True)\n",
    "    # make a copy of the features that removes the removed labels from ebfore\n",
    "    features = copy.copy(features_raw)\n",
    "    features.drop(columns='global_id', inplace=True)\n",
    "    features.drop(columns='timestep', inplace=True)\n",
    "    features.drop(columns='section', inplace=True)\n",
    "    features.drop(columns='spatial_id', inplace=True)\n",
    "    features.drop(columns='x', inplace=True)\n",
    "    features.drop(columns='y', inplace=True)\n",
    "    # grab the values from both edited objects \n",
    "    features = features.values\n",
    "    labels   = labels_raw.values\n",
    "    # convert column labels into a list to grab the feature names and remove the irrelevant feature names, grab label names\n",
    "    feature_names = list(features_raw.columns)\n",
    "    feature_names.remove('global_id')\n",
    "    feature_names.remove('timestep')\n",
    "    feature_names.remove('section')\n",
    "    feature_names.remove('spatial_id')\n",
    "    feature_names.remove('x')\n",
    "    feature_names.remove('y')\n",
    "    # label_names = list(labels_raw.columns)\n",
    "    log.write_line(\"additional values generated successfully.\")\n",
    "       \n",
    "    # get integer classifier count\n",
    "    classifier_count = getenv('NUM_CLASSIFIERS')\n",
    "    classifier_count = int(classifier_count)\n",
    "    log.write_line(\"got classifier count.\")\n",
    "    \n",
    "    # create model class instance\n",
    "    model = SSGModel(classifier_model, classifier_count, pred_start_timestep, log)\n",
    "    log.write_line(\"model instantiated.\")\n",
    "    \n",
    "    # train the model\n",
    "    train_x, predict_x, store_columns = model.train(TRAIN_SECTION, int(getenv('TEST_SECTION')), features_raw, features, feature_names, labels, trap_effort, section_col, geo_features_csv)\n",
    "    log.write_line(\"model trained successfully.\")\n",
    "\n",
    "    # log and make predictions for given input file range\n",
    "    predictions, variances = model.predict(TRAIN_SECTION, train_x, predict_x, store_columns)\n",
    "    log.write_line(\"predictions made successfully.\")\n",
    "\n",
    "    # add predictions to predictions directory\n",
    "    predictions.to_csv(path.join(write_dir, 'predictions.csv'))\n",
    "    log.write_line(\"predictions saved successfully.\")\n",
    "\n",
    "    # add variances of those predictions to the write directory if desired --> TODO Sus\n",
    "    if variances:\n",
    "        variances_csv_str = variances.to_csv(index=False, encoding='utf-8')\n",
    "        variances_csv_str.to_csv(path.join(write_dir, 'variances.csv'))\n",
    "        log.write_line(\"variances saved successfully.\")\n",
    "\n",
    "    # log completion\n",
    "    log.write_line(\"predictions completed.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## full API function ##\n",
    "\n",
    "def invasive_species_trap_placement_api(log):\n",
    "    data = traps_get_data(log)\n",
    "    log.write_line(\"Finished grabbing data.\")\n",
    "    \n",
    "    validated_data = traps_validate_data(log)\n",
    "    log.write_line(\"Finished validating data.\")\n",
    "\n",
    "    preprocessed_data = traps_preprocess(log)\n",
    "    log.write_line(\"Finished preprocessing data.\")\n",
    "\n",
    "    consolidated_data = traps_consolidate(log)\n",
    "    log.write_line(\"Finished consolidating data.\")\n",
    "\n",
    "    predictions = traps_predict(log)\n",
    "    log.write_line(\"Finished predicting trap locations.\")\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the code ##\n",
    "\n",
    "# setup log\n",
    "log = Log(\"../Logs/log.txt\")\n",
    "log.write_line(\"Initialized log.\")\n",
    "\n",
    "# run pipeline\n",
    "predictions = invasive_species_trap_placement_api(log)\n",
    "log.write_line(\"Finished predicting, closing log.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAWS Implementations To Annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## IWARE MODEL FROM PAWS ##\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m minimize\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "## IWARE MODEL FROM PAWS ##\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "# from iware.gpc import GaussianProcessClassifier\n",
    "\n",
    "from os import getenv\n",
    "from multiprocessing import Pool, get_context\n",
    "from functools import partial\n",
    "from os import getpid\n",
    "\n",
    "# from opencensus.ext.azure import metrics_exporter\n",
    "# from opencensus.stats import aggregation as aggregation_module\n",
    "# from opencensus.stats import measure as measure_module\n",
    "# from opencensus.stats import stats as stats_module\n",
    "# from opencensus.stats import view as view_module\n",
    "# from opencensus.tags import tag_map as tag_map_module\n",
    "# from opencensus.tags import tag_key as tag_key_module\n",
    "# from opencensus.tags import tag_value as tag_value_module\n",
    "\n",
    "# appinsights_key = getenv('APPINSIGHTS_INSTRUMENTATIONKEY', None)\n",
    "\n",
    "POSITIVE_LABEL = 1      # how a positive label is encoded in the data\n",
    "RANDOM_SEED = None        # could be None\n",
    "N_JOBS = 1 # -1 to use max\n",
    "\n",
    "# parameters for bagging classifier\n",
    "NUM_ESTIMATORS = 32 #32 #50\n",
    "MAX_SAMPLES = 0.8\n",
    "MAX_FEATURES = .5\n",
    "\n",
    "# verbose output if == 1\n",
    "VERBOSE = 0\n",
    "\n",
    "MULTIPROCESSING_POOL_SIZE_ENV_VAR='MULTIPROCESSING_POOL_SIZE'\n",
    "\n",
    "###########################################################\n",
    "# utility functions\n",
    "###########################################################\n",
    "# given training and predict sets, normalize data to zero mean, unit variance\n",
    "def normalize_data(train, predict):\n",
    "    scaler = StandardScaler()\n",
    "    # fit only on training data\n",
    "    scaler.fit(train)\n",
    "    # apply normalization to training and test data\n",
    "    train = scaler.transform(train)\n",
    "    predict = scaler.transform(predict)\n",
    "\n",
    "    return train, predict\n",
    "\n",
    "# by maximizing F1 score?\n",
    "def determine_threshold(label, predict_test_pos_probs, num_thresholds=50):\n",
    "    # TODO: previously, used tpr-(1-fpr)\n",
    "    # fpr, tpr, thresholds = metrics.roc_curve(label, predict_test_pos_probs, pos_label=POSITIVE_LABEL)\n",
    "    # or maybe scaled, like 2*tpr - (1-fpr)?\n",
    "\n",
    "    thresholds = np.linspace(0, 1, num_thresholds)\n",
    "    f1         = np.zeros(thresholds.size)\n",
    "    precision  = np.zeros(thresholds.size)\n",
    "    recall     = np.zeros(thresholds.size)\n",
    "    auprc      = np.zeros(thresholds.size)\n",
    "\n",
    "    for i in range(num_thresholds):\n",
    "        predict_labels = predict_test_pos_probs > thresholds[i]\n",
    "        predict_labels = predict_labels.astype(int)\n",
    "\n",
    "        f1[i]        = metrics.f1_score(label, predict_labels)\n",
    "        precision[i] = metrics.precision_score(label, predict_labels, pos_label=POSITIVE_LABEL)\n",
    "        recall[i]    = metrics.recall_score(label, predict_labels, pos_label=POSITIVE_LABEL)\n",
    "\n",
    "        precision_vals, recall_vals, _ = metrics.precision_recall_curve(label, predict_test_pos_probs, pos_label=POSITIVE_LABEL)\n",
    "        auprc[i]     = metrics.auc(recall_vals, precision_vals)\n",
    "\n",
    "        if VERBOSE:\n",
    "            print('threshold: {:.4f} | f1: {:.4f},  precision: {:.4f}, recall: {:.4f}, AUPRC: {:.4f}'.format(thresholds[i], f1[i], precision[i], recall[i], auprc[i]))\n",
    "\n",
    "    # opt = np.argmax(f1)\n",
    "    opt = np.argmax(auprc)\n",
    "    print('optimal threshold {:.4f}, with f1 {:.4f}, precision {:.4f}, recall {:.4f}, AUPRC {:.4f}'.format(thresholds[opt], f1[opt], precision[opt], recall[opt], auprc[opt]))\n",
    "\n",
    "    return thresholds[opt]\n",
    "\n",
    "# get classifier used as base estimator in bagging classifier\n",
    "def get_base_estimator(method):\n",
    "    if method == 'gaussian_processes':\n",
    "        kernel = 1.0 * RBF(length_scale=1.0)\n",
    "        base_estimator = GaussianProcessClassifier(kernel=kernel, random_state=RANDOM_SEED, warm_start=True, max_iter_predict=100, n_jobs=-1)\n",
    "    elif method == 'svm':\n",
    "        base_estimator = SVC(gamma='auto', random_state=RANDOM_SEED)\n",
    "    elif method == 'linear-svc':\n",
    "        base_estimator = LinearSVC(max_iter=5000, random_state=RANDOM_SEED)\n",
    "    elif method == 'decision_tree':\n",
    "        base_estimator = tree.DecisionTreeClassifier(random_state=RANDOM_SEED)\n",
    "    else:\n",
    "        raise Exception('method \\'{}\\' not recognized'.format(method))\n",
    "\n",
    "    return base_estimator\n",
    "\n",
    "\n",
    "# get overall classifier to use\n",
    "def get_classifier(use_balanced, method):\n",
    "    if method == 'random_forest':\n",
    "        return RandomForestClassifier(n_estimators=NUM_ESTIMATORS,\n",
    "            criterion='gini', max_depth=None, min_samples_split=2,\n",
    "            min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "            max_features=MAX_FEATURES, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            bootstrap=True, oob_score=False, n_jobs=N_JOBS,\n",
    "            random_state=RANDOM_SEED, verbose=VERBOSE,\n",
    "            warm_start=False, class_weight=None)\n",
    "\n",
    "    base_estimator = get_base_estimator(method)\n",
    "\n",
    "    if method == 'gaussian_processes':\n",
    "        # gaussian_processess don't need a bagging classifier\n",
    "        return base_estimator\n",
    "    elif use_balanced:\n",
    "        # balanced bagging classifier used for datasets with strong label imbalance\n",
    "        return BalancedBaggingClassifier(base_estimator=base_estimator,\n",
    "            n_estimators=NUM_ESTIMATORS, max_samples=MAX_SAMPLES,\n",
    "            max_features=MAX_FEATURES,\n",
    "            bootstrap=True, bootstrap_features=False,\n",
    "            oob_score=False, warm_start=False,\n",
    "            sampling_strategy='majority', #sampling_strategy=0.8,\n",
    "            replacement=True, n_jobs=N_JOBS,\n",
    "            random_state=RANDOM_SEED, verbose=VERBOSE)\n",
    "    else:\n",
    "        # non-balanced bagging classifier used for other datasets\n",
    "        return BaggingClassifier(base_estimator=base_estimator,\n",
    "            n_estimators=NUM_ESTIMATORS, max_samples=MAX_SAMPLES,\n",
    "            max_features=MAX_FEATURES,\n",
    "            bootstrap=True, bootstrap_features=False,\n",
    "            oob_score=False, warm_start=False, n_jobs=N_JOBS,\n",
    "            random_state=RANDOM_SEED, verbose=VERBOSE)\n",
    "\n",
    "def train_single_classifier(patrol_threshold, use_balanced, method, train_x, train_y, train_effort):\n",
    "    # reduce to size desired\n",
    "    train_y = np.squeeze(train_y)\n",
    "    \n",
    "    # idx = np.where(np.logical_or(train_effort >= patrol_threshold, train_y == POSITIVE_LABEL))[0]\n",
    "\n",
    "    # print('train_effort', train_effort)\n",
    "    # print('patrol_threshold', patrol_threshold)\n",
    "    # print('train_y', train_y)\n",
    "    # print('train_y.shape', train_y.shape)\n",
    "    # print('reshaped train_y', train_y)\n",
    "    # print('reshaped train_y shape', train_y.shape)\n",
    "    # print('POSITIVE_LABEL', POSITIVE_LABEL)\n",
    "\n",
    "    # print('type', type(train_y))\n",
    "\n",
    "    # print(np.where(train_effort >= patrol_threshold)[0].shape)\n",
    "    # print(np.where(train_y == POSITIVE_LABEL)[0].shape)\n",
    "\n",
    "    # print('np.logical or', np.logical_or(train_effort >= patrol_threshold, train_y == POSITIVE_LABEL))\n",
    "    # print('np.logical or shape', np.logical_or(train_effort >= patrol_threshold, train_y == POSITIVE_LABEL).shape)\n",
    "    # print('np.where', np.where(np.logical_or(train_effort >= patrol_threshold, train_y == POSITIVE_LABEL)))\n",
    "    # # print('np.where shape', np.where(np.logical_or(train_effort >= patrol_threshold, train_y == POSITIVE_LABEL)).shape)\n",
    "    # print('np.where[0] shape', np.where(np.logical_or(train_effort >= patrol_threshold, train_y == POSITIVE_LABEL))[0].shape)\n",
    "    # # print('np.where[1] shape', np.where(np.logical_or(train_effort >= patrol_threshold, train_y == POSITIVE_LABEL))[1].shape)\n",
    "\n",
    "\n",
    "    # idx_where_train_effort = np.where(train_effort >= patrol_threshold)[0]\n",
    "    # print('idx_where_train_effort', idx_where_train_effort.shape)\n",
    "\n",
    "    # idx_where_positive = np.where(train_y == POSITIVE_LABEL)[0]\n",
    "    # print('idx_where_positive', idx_where_positive.shape)\n",
    "\n",
    "    # get index of where to cut off points to analyze where either of these expressions holds (positive label or effort is beyond/at threshold)\n",
    "    idx = np.where(np.logical_or(train_effort >= patrol_threshold, train_y == POSITIVE_LABEL))[0]\n",
    "    \n",
    "    # print('idx shape', idx.shape)\n",
    "    # # print('patrol threshold', patrol_threshold)\n",
    "    # print('train effort', train_effort.shape)\n",
    "    # print('num positive train_y', np.sum(train_y))\n",
    "    # print(' **** Training classifier for threshold: ' + str(patrol_threshold))\n",
    "    # print(\"pid: \", getpid())\n",
    "\n",
    "    # if idx has no points for a given training threshold nothing can happen\n",
    "    if idx.size == 0:\n",
    "        print('no training points found for threshold = {}'.format(patrol_threshold))\n",
    "        return None\n",
    "\n",
    "    # filter data for given idx \n",
    "    train_x_filter = train_x[idx, :]\n",
    "    train_y_filter = train_y[idx]\n",
    "\n",
    "    print('filtered data: {}. num positive labels {}. threshold {}'.format(train_x_filter.shape, np.sum(train_y_filter), patrol_threshold))\n",
    "\n",
    "    # filtered labels have no positive hits, threshold not useful to train on.\n",
    "    if np.sum(train_y_filter) == 0:\n",
    "        print('no positive labels in this subset of the training data. skipping threshold {}'.format(patrol_threshold))\n",
    "        return None\n",
    "\n",
    "    # print('threshold {}, num x {}'.format(patrol_threshold, train_x_filter.shape))\n",
    "\n",
    "    # # print('before raveled', train_y_filter.shape)\n",
    "    # train_y_filter = train_y_filter.ravel() # to get rid of DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
    "    # # print('after raveled', train_y_filter.shape)\n",
    "    # print('ravel train_y_filter done')\n",
    "\n",
    "    # print('train x filter', train_x_filter)\n",
    "    # print('train y filter', train_y_filter)\n",
    "\n",
    "    # fit training data, get classifier and fit function according to method using get_classifier fn\n",
    "    classifier = get_classifier(use_balanced, method)\n",
    "    # print('get_classifier done')\n",
    "    # print('train_x_filter shape', train_x_filter.shape)\n",
    "    # print('train_y_filter shape', train_y_filter.shape)\n",
    "    classifier.fit(train_x_filter, train_y_filter)\n",
    "\n",
    "    # print('single classifier done fitting')\n",
    "\n",
    "    # return fitted classifier\n",
    "    return classifier\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# iWare-E class\n",
    "###########################################################\n",
    "class iWare:\n",
    "    def __init__(self, method, num_classifiers, year, task_id):\n",
    "        self.method = method\n",
    "        self.num_classifiers = num_classifiers\n",
    "        self.year = year\n",
    "        self.patrol_thresholds = None\n",
    "        self.classifiers = None\n",
    "        self.weights = None         # weights for classifiers\n",
    "\n",
    "        self.classifier_train_measure = None\n",
    "        self.stats_recorder = None\n",
    "        self.key_patrol_threshold = None\n",
    "        self.classifier_train_tmap = None\n",
    "\n",
    "        # optional settings to monitor stats on runtime performance of classifier (training duration etc.)\n",
    "        # if (appinsights_key):\n",
    "        #     stats = stats_module.stats\n",
    "        #     view_manager = stats.view_manager\n",
    "        #     self.stats_recorder = stats.stats_recorder\n",
    "\n",
    "        #     self.classifier_train_measure = measure_module.MeasureFloat(\"ClassifierTrainDuration\",\n",
    "        #                                                 \"Classifier train duration in seconds\",\n",
    "        #                                                 \"seconds\")\n",
    "\n",
    "        #     key_task_id = tag_key_module.TagKey(\"task_id\")\n",
    "        #     CLASSIFIER_TRAIN_VIEW = view_module.View(\"ClassifierTrainDuration\",\n",
    "        #                                         \"Classifier train duration in seconds\",\n",
    "        #                                         [key_task_id],\n",
    "        #                                         self.classifier_train_measure,\n",
    "        #                                         aggregation_module.LastValueAggregation())\n",
    "\n",
    "        #     self.classifier_train_tmap = tag_map_module.TagMap()\n",
    "        #     self.classifier_train_tmap.insert(key_task_id, tag_value_module.TagValue(task_id))\n",
    "\n",
    "\n",
    "    ###########################################################\n",
    "    # classification\n",
    "    ###########################################################\n",
    "\n",
    "    def get_patrol_thresholds(self, train_effort):\n",
    "        patrol_threshold_percentile = np.linspace(0, 100, self.num_classifiers, endpoint=False)\n",
    "        patrol_thresholds = np.percentile(train_effort, patrol_threshold_percentile)\n",
    "        print('percentiles {}'.format(patrol_threshold_percentile))\n",
    "        print('patrol thresholds {}'.format(patrol_thresholds))\n",
    "        return patrol_thresholds\n",
    "\n",
    "    # currently does not use cross validation? or enable tuning of the V_p hyperparameter?\n",
    "    # currently only does trivial case of identity matrix for the final combined matrix?\n",
    "    def get_vote_matrix(self):\n",
    "        vote_power = np.identity(self.num_classifiers)                           # identity matrix\n",
    "        vote_qual = np.ones((self.num_classifiers, self.num_classifiers))\n",
    "\n",
    "        # create combined vote matrix\n",
    "        vote_combine = np.multiply(vote_power, vote_qual)\n",
    "\n",
    "        # normalize column-wise\n",
    "        vote_combine = vote_combine / vote_combine.sum(1)[:,None]\n",
    "\n",
    "        return vote_combine\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # train a set of classifiers using provided data\n",
    "    def train_classifiers(self, patrol_thresholds, train_x, train_y, train_effort, use_balanced):\n",
    "        classifiers = []\n",
    "        pool_size = getenv(MULTIPROCESSING_POOL_SIZE_ENV_VAR, None)\n",
    "        print('pool size', pool_size)\n",
    "        print('Training ' + str(len(patrol_thresholds)) + ' classifiers with a pool size of ' + str(pool_size))\n",
    "\n",
    "        with get_context(\"spawn\").Pool(processes=int(pool_size)) as pool:\n",
    "            # time process\n",
    "            start_time = time.time()\n",
    "            # get classifiers by training each single classifier\n",
    "            classifiers = pool.map(partial(train_single_classifier, use_balanced=use_balanced, method=self.method, train_x=train_x, train_y=train_y, train_effort=train_effort), patrol_thresholds)\n",
    "            # get analytics: finish timing\n",
    "            # if (appinsights_key):\n",
    "            #     dur = (time.time() - start_time)\n",
    "            #     print('Training duration: ' + str(dur))\n",
    "            #     mmap = self.stats_recorder.new_measurement_map()\n",
    "            #     mmap.measure_float_put(self.classifier_train_measure, dur)\n",
    "            #     mmap.record(self.classifier_train_tmap)\n",
    "        print('all train_single_classifiers done')\n",
    "        # return results\n",
    "        return classifiers\n",
    "\n",
    "        # print('patrol thresholds', patrol_thresholds)\n",
    "        # classifiers = []\n",
    "        # for patrol_threshold in patrol_thresholds:\n",
    "        #     trained_classifier = train_single_classifier(patrol_threshold, use_balanced, self.method, train_x, train_y, train_effort)\n",
    "        #     print('trained classifier', trained_classifier)\n",
    "        #     classifiers.append(trained_classifier)\n",
    "        # print('all train_single_classifiers done')\n",
    "        # return classifiers\n",
    "\n",
    "    # training classifiers within the model\n",
    "    def train_iware(self, all_train_x, all_train_y, all_train_effort, use_balanced=False, nsplits=5):\n",
    "        self.patrol_thresholds = self.get_patrol_thresholds(all_train_effort)\n",
    "\n",
    "        print('shape x', all_train_x.shape)\n",
    "        print('shape y', all_train_y.shape)\n",
    "        print('shape train_effort', all_train_effort.shape)\n",
    "\n",
    "        self.weights = self.get_vote_matrix()\n",
    "\n",
    "        print('-------------------------------------------')\n",
    "        print('training classifiers with all train data')\n",
    "        print('-------------------------------------------')\n",
    "\n",
    "        self.classifiers = self.train_classifiers(self.patrol_thresholds, all_train_x, all_train_y, all_train_effort, use_balanced)\n",
    "        print('done train_iware')\n",
    "\n",
    "    ###########################################################\n",
    "    # iWare-E for predicting future risk\n",
    "    ###########################################################\n",
    "    def train(self, predict_section, predict_section_test, features_raw, features, feature_names,\n",
    "            labels, patrol_effort, section_col, input_static_feats,\n",
    "            test_temp=None, test_precip=None, gaussian_processesp_filename=None):\n",
    "        predict_year = self.year\n",
    "        # ----------------------------------------------\n",
    "        # get training data\n",
    "        # ----------------------------------------------\n",
    "        # use all data before specified (predict_year, predict_section)\n",
    "        train_idx = np.where(np.logical_or(features_raw['year'] < predict_year,\n",
    "            np.logical_and(features_raw['year'] == predict_year, features_raw['section'] < predict_section_test)))[0]\n",
    "\n",
    "        train_x = features[train_idx, :]\n",
    "        train_y = labels[train_idx]\n",
    "        train_patrol_effort = patrol_effort[train_idx]\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # get data to predict on\n",
    "        # ----------------------------------------------\n",
    "        if predict_section == 0:\n",
    "            prev_year = predict_year - 1\n",
    "            num_section = np.max(section_col)\n",
    "            prev_section = num_section\n",
    "        else:\n",
    "            prev_year = predict_year\n",
    "            prev_section = predict_section - 1\n",
    "\n",
    "        print('  test section: year {}, section {}'.format(predict_year, predict_section))\n",
    "        print('  prev section: year {}, section {}'.format(prev_year, prev_section))\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # set up data arrays\n",
    "        # ----------------------------------------------\n",
    "        # get past patrol effort for the test section\n",
    "        prev_section_idx = np.where(np.logical_and(features_raw['year'] == prev_year, features_raw['section'] == prev_section))\n",
    "        past_patrol_effort = patrol_effort[prev_section_idx]\n",
    "\n",
    "        prev_section_spatial_id = features_raw['spatial_id'].values[prev_section_idx]\n",
    "        patrol_effort_df = pd.DataFrame({'spatial_id': prev_section_spatial_id,\n",
    "                                            'past_patrol_effort': past_patrol_effort})\n",
    "\n",
    "        # get all static features\n",
    "        # input_static_featsa = list(input_static_feats.columns)\n",
    "        # patrol_effort_dfa = list(patrol_effort_df.columns)\n",
    "\n",
    "\n",
    "        #input_static_feats.drop(columns=input_static_feats.columns[0], inplace=True)\n",
    "        # create features array and add in past_patrol_effort\n",
    "        predict_x_df = input_static_feats.join(patrol_effort_df.set_index('spatial_id'), on='spatial_id', how='left')\n",
    "\n",
    "        # input_static_featsa = list(predict_x_df.columns)\n",
    "\n",
    "        predict_x_df['past_patrol_effort'].fillna(0, inplace=True)\n",
    "\n",
    "        print(predict_x_df)\n",
    "\n",
    "        # add climate info\n",
    "        if test_temp is not None and test_precip is not None:\n",
    "            predict_x_df['temp']   = test_temp * np.ones(input_static_feats.shape[0])\n",
    "            predict_x_df['precip'] = test_precip * np.ones(input_static_feats.shape[0])\n",
    "\n",
    "        # add gaussian_processes info\n",
    "        if gaussian_processesp_filename is not None:\n",
    "            new_gaussian_processesp = pd.read_csv('../preprocess_consolidate/belum_traponly_combined/1000/output/all_3month/gaussian_processesP_2019_0.csv')\n",
    "            predict_x_df['gaussian_processesp'] = new_gaussian_processesp['2019-0']\n",
    "\n",
    "        # arrange columns to match training data\n",
    "        store_columns = predict_x_df[['spatial_id', 'x', 'y']]\n",
    "        predict_x_df.drop(columns=['spatial_id', 'x', 'y'], inplace=True)\n",
    "\n",
    "        predict_x_df = predict_x_df[feature_names]\n",
    "        predict_x = predict_x_df.values\n",
    "\n",
    "        # normalize data\n",
    "        train_x, predict_x = normalize_data(train_x, predict_x)\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # train classifiers\n",
    "        # ----------------------------------------------\n",
    "        print('training classifiers on {} points...'.format(train_x.shape))\n",
    "\n",
    "        train_start_time = time.time()\n",
    "        self.train_iware(train_x, train_y, train_patrol_effort)\n",
    "        total_train_time = time.time() - train_start_time\n",
    "        print('total train time {:.3f}'.format(total_train_time))\n",
    "        return train_x, predict_x, store_columns\n",
    "\n",
    "    # use all provided data to make predictions\n",
    "    def make_predictions(self, predict_section, train_x, predict_x, store_columns):\n",
    "        # ----------------------------------------------\n",
    "        # run classifiers to get set of predictions\n",
    "        # ----------------------------------------------\n",
    "        # intiialize array to store predictions from each classifier\n",
    "        predict_year = self.year\n",
    "        print('making predictions on year {} section {}... {} points'.format(predict_year, predict_section, predict_x.shape))\n",
    "        final_predictions = np.zeros((predict_x.shape[0], self.num_classifiers))\n",
    "\n",
    "        print('final_predictions', final_predictions)\n",
    "\n",
    "        if self.method == 'gaussian_processes' or self.method == 'random_forest':\n",
    "            final_variances = np.zeros((predict_x.shape[0], self.num_classifiers))\n",
    "\n",
    "        print('self.num_classifiers', self.num_classifiers)\n",
    "\n",
    "        print('0')\n",
    "\n",
    "        # make predictions with each classifier\n",
    "        for i in range(self.num_classifiers):\n",
    "\n",
    "            print('')\n",
    "            print('ITERATION', i)\n",
    "            print('')\n",
    "\n",
    "            # this classifier had no training points, so we skip it\n",
    "            if self.classifiers[i] is None:\n",
    "                final_predictions[:, i] = np.zeros((final_predictions.shape[0]))\n",
    "                continue\n",
    "\n",
    "            print('self.classifiers[i]', self.classifiers[i])\n",
    "\n",
    "            if self.method == 'gaussian_processes' or self.method == 'random_forest':\n",
    "                if self.method == 'random_forest':\n",
    "                    curr_predictions, curr_variances = self.classifiers[i].predict_proba(predict_x, return_var=True, train_x=train_x)\n",
    "                else:\n",
    "                    curr_predictions = self.classifiers[i].predict_proba(predict_x)\n",
    "                    curr_variances = self.classifiers[i].predict_var(predict_x)\n",
    "\n",
    "                print('variance min {} max {}'.format(np.min(curr_variances), np.max(curr_variances)))\n",
    "\n",
    "                final_variances[:, i] = curr_variances\n",
    "            else:\n",
    "                curr_predictions = self.classifiers[i].predict_proba(predict_x)\n",
    "\n",
    "\n",
    "            # curr_predictions has two columns, first column for probability of 0, second col for prob of 1\n",
    "            # if on classifier iter 8 and 9, number of filtered datapoints = number of positive illegal activity\n",
    "            # this is because for higher patrol thresholds, there may not be any datapoints from train_x that have greater patrol effort\n",
    "            # so for the np.where constructing filtered data, the selected data is just the illegal data,\n",
    "            # so we can predict 100% confidence those have illegal instances\n",
    "            # so for iter 8 and 9 it's just 1 column because the prob is all 1 so it'll be e.g. (97,1) rather than (97,2)\n",
    "\n",
    "            # print('curr_predictions', curr_predictions)\n",
    "            print(np.size(curr_predictions))\n",
    "\n",
    "            # for (n,1) case, where all n probabilities are 1\n",
    "            if np.size(curr_predictions) == curr_predictions.shape[0]:\n",
    "                curr_predictions = np.ravel(curr_predictions)\n",
    "            # for (n,2) case, first column for probability of 0, second col for prob of 1\n",
    "            else:\n",
    "                print('in')\n",
    "                curr_predictions = curr_predictions[:, 1]   # probability of positive label\n",
    "\n",
    "            final_predictions[:, i] = curr_predictions\n",
    "\n",
    "            print('final_predictions', final_predictions)\n",
    "\n",
    "        # save out predictions to CSV\n",
    "        print('  save out predictions...')\n",
    "        predictions_df = pd.DataFrame(data=final_predictions, columns=['threshold={}'.format(thresh) for thresh in self.patrol_thresholds])\n",
    "\n",
    "        predictions_df = pd.concat([store_columns, predictions_df], axis=1)\n",
    "\n",
    "        # save out variances to CSV\n",
    "        variances_df = None\n",
    "        if self.method == 'gaussian_processes' or self.method == 'random_forest':\n",
    "            print('  save out variances...')\n",
    "            variances_df = pd.DataFrame(data=final_variances, columns=['threshold={}'.format(thresh) for thresh in self.patrol_thresholds])\n",
    "            variances_df = pd.concat([store_columns, variances_df], axis=1)\n",
    "        return predictions_df, variances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREDICTION FROM PAWS ##\n",
    "# predictions.py: defines functions to run predictions and do data setup within the prediction generation\n",
    "\n",
    "# imports\n",
    "# from iware.iware import * # custom package -> need to determine what to use here\n",
    "# from iware import * # custom package -> need to determine what to use here\n",
    "import calendar # default package\n",
    "import copy # default package\n",
    "\n",
    "# cuts useless stuff from big data frame before prediction\n",
    "def setup_data(features_raw, labels_raw):\n",
    "    # TODO mod to be trapping effort\n",
    "    patrol_effort = features_raw['current_patrol_effort'].values\n",
    "    section_col   = features_raw['section'].values\n",
    "    year_col      = features_raw['year'].values\n",
    "\n",
    "    # drop first column and the current_patrol_effort after grabbing the values and the corresponding first col label\n",
    "    features_raw.drop(columns=features_raw.columns[0], inplace=True)\n",
    "    features_raw.drop(columns='current_patrol_effort', inplace=True)\n",
    "    labels_raw.drop(columns=labels_raw.columns[0], inplace=True)\n",
    "\n",
    "    # get rid of following column labels \n",
    "    labels_raw.drop(columns='global_id', inplace=True)\n",
    "    labels_raw.drop(columns='year', inplace=True)\n",
    "    labels_raw.drop(columns='section', inplace=True)\n",
    "    labels_raw.drop(columns='spatial_id', inplace=True)\n",
    "    labels_raw.drop(columns='x', inplace=True)\n",
    "    labels_raw.drop(columns='y', inplace=True)\n",
    "\n",
    "    # make a copy of the features that removes the removed labels from ebfore\n",
    "    features = copy.copy(features_raw)\n",
    "    features.drop(columns='global_id', inplace=True)\n",
    "    features.drop(columns='year', inplace=True)\n",
    "    features.drop(columns='section', inplace=True)\n",
    "    features.drop(columns='spatial_id', inplace=True)\n",
    "    features.drop(columns='x', inplace=True)\n",
    "    features.drop(columns='y', inplace=True)\n",
    "\n",
    "    # grab the values from both edited objects \n",
    "    features = features.values\n",
    "    labels   = labels_raw.values\n",
    "\n",
    "    # convert column labels into a list to grab the feature names and remove the irrelevant feature names, grab label names\n",
    "    feature_names = list(features_raw.columns)\n",
    "    feature_names.remove('global_id')\n",
    "    feature_names.remove('year')\n",
    "    feature_names.remove('section')\n",
    "    feature_names.remove('spatial_id')\n",
    "    feature_names.remove('x')\n",
    "    feature_names.remove('y')\n",
    "    label_names = list(labels_raw.columns)\n",
    "\n",
    "    # check feature and label names are converted properly\n",
    "    print('feature names {}'.format(feature_names))\n",
    "    print('label names {}'.format(label_names))\n",
    "\n",
    "    # check shapes of all feature objects created are what we expected\n",
    "    print('features_raw: ' + str(np.shape(features_raw)))\n",
    "    print('features: ' + str(np.shape(features)))\n",
    "    print('feature_names: ' + str(np.shape(feature_names)))\n",
    "    # return necessary data\n",
    "    return features_raw, features, feature_names, labels, patrol_effort, section_col, year_col\n",
    "\n",
    "\n",
    "\n",
    "# making predictions runner function\n",
    "# models accepted: decision_tree, random_forest, gaussian_processes\n",
    "        # temporal_training_resolution_month_count: used to inform the number of datapoints in a month, tells us how many datapoints are attached to a given month\n",
    "def run_predictions(TRAIN_SECTION, log, taskId,\n",
    "                    dataIO_helper,\n",
    "                    months_prefix,\n",
    "                    temporal_training_resolution_month_count,\n",
    "                    classifier_model,\n",
    "                    prediction_start_year,\n",
    "                    container_name,\n",
    "                    run_id):\n",
    "    # find number of sections\n",
    "    section_count = int(12 / temporal_training_resolution_month_count)\n",
    "    \n",
    "    # gather the data into its final format\n",
    "    get_top_directory = 'post_processed_data'\n",
    "    path = '{}_{}month'.format(months_prefix, temporal_training_resolution_month_count)\n",
    "    x_filename = '{}_x.csv'.format(months_prefix)\n",
    "    features_raw = dataIO_helper.get_csv(x_filename, get_top_directory, path)\n",
    "\n",
    "    path = '{}_{}month'.format(months_prefix, temporal_training_resolution_month_count)\n",
    "    y_filename = '{}_y.csv'.format(months_prefix)\n",
    "    labels_raw = dataIO_helper.get_csv(y_filename, get_top_directory, path)\n",
    "\n",
    "    log.log_info('predict: {}'.format('Running setup_data.'), taskId=taskId)\n",
    "    features_raw, features, feature_names, labels, patrol_effort, section_col, year_col = setup_data(features_raw, labels_raw)\n",
    "    # get the classifier count \n",
    "    classifier_count = getenv('NUM_CLASSIFIERS')\n",
    "    classifier_count = int(classifier_count)\n",
    "\n",
    "    # create a model instance\n",
    "    iware_model = iWare(classifier_model, classifier_count, prediction_start_year, taskId)\n",
    "\n",
    "    # Train model #\n",
    "    # get static features\n",
    "    geo_features_csv = dataIO_helper.get_csv('static_features.csv', get_top_directory)\n",
    "\n",
    "    # train the model\n",
    "    train_x, predict_x, store_columns = iware_model.train(TRAIN_SECTION, int(getenv('TEST_SECTION')), features_raw, features, feature_names, labels, patrol_effort, section_col, geo_features_csv)\n",
    "\n",
    "    # iterate over number of sections\n",
    "    for section in range(1, section_count + 1):\n",
    "        # find month range\n",
    "        month_end_num = temporal_training_resolution_month_count * section\n",
    "        month_end = calendar.month_name[month_end_num]\n",
    "        month_start_num = month_end_num - (temporal_training_resolution_month_count - 1)\n",
    "        month_start = calendar.month_name[month_start_num]\n",
    "\n",
    "        # log and make predictions for given month range\n",
    "        log.log_info('predict: {}'.format('Making predictions for section: {} - {}-{}'.format(str(section), month_start, month_end)), taskId=taskId)\n",
    "        print('Making predictions for section: {} - {}-{}'.format(str(section), month_start, month_end))\n",
    "\n",
    "        predictions, variances = iware_model.make_predictions(section, train_x, predict_x, store_columns)\n",
    "\n",
    "        # add predictions to predictions directory\n",
    "        write_dir = 'risk_prediction'\n",
    "        if section_count == 1:\n",
    "            dataIO_helper.write_csv(predictions, 'predictions_{}.csv'.format(prediction_start_year), write_dir)\n",
    "        else:\n",
    "            dataIO_helper.write_csv(predictions, 'predictions_{}-{}_{}.csv'.format(month_start, month_end, prediction_start_year), write_dir)\n",
    "\n",
    "        # add variances of those predictions to the write directory if desired\n",
    "        if variances:\n",
    "            variances_csv_str = variances.to_csv(index=False, encoding='utf-8')\n",
    "            if section_count == 1:\n",
    "                dataIO_helper.write_csv(variances_csv_str, 'variances_{}.csv'.format(prediction_start_year), write_dir)\n",
    "            else:\n",
    "                month_end = calendar.month_name[temporal_training_resolution_month_count * section]\n",
    "                month_start = calendar.month_name[month_end - temporal_training_resolution_month_count + 1]\n",
    "                dataIO_helper.write_csv(variances_csv_str, 'variances_{}-{}_{}.csv'.format(month_start, month_end, prediction_start_year), write_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
