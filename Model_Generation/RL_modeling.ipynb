{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done importing\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "import synthetic_data\n",
    "# import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "print(\"done importing\")\n",
    "\n",
    "class TrapEnvironment:\n",
    "    def __init__(self, num_traps=20,max_steps=100):\n",
    "        # Initialize environment parameters\n",
    "        self.num_traps = num_traps\n",
    "        self.n = 25\n",
    "        self.m = 25\n",
    "        self.predator_density = None\n",
    "        self.prey_density = None\n",
    "        self.trap_replacement_rate = 10\n",
    "        self.pts_per_sec = 100\n",
    "        self.len_traj=50\n",
    "        self.current_step = 0\n",
    "        self.max_steps = max_steps\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment to its initial state (just predator and prey)\n",
    "        # Generate initial predator and prey densities\n",
    "        # default params for initial generation num_traj=200,len_traj=50, pts_per_sec=100, save_loc='../Data/val.npy', prey_range=(1, 5), predator_range=(1, 3)\n",
    "        data_init = synthetic_data.generate().reshape(self.n, self.m, 2, self.pts_per_sec*self.len_traj)\n",
    "        self.prey_density, self.predator_density = data_init[:, :, 0, -1], data_init[:, :, 1, -1]\n",
    "        return self.predator_density, self.prey_density\n",
    "\n",
    "    def step(self, action):\n",
    "        # Take action (place traps) and observe the next state and reward\n",
    "        # Update predator and prey densities based on the action\n",
    "        # Calculate reward based on the change in predator density\n",
    "        # Return next state, reward, and done flag\n",
    "\n",
    "        # Simulate predator dynamics with traps placed at specified locations\n",
    "        # Here, action is a list of trap locations [(i1, j1), (i2, j2), ..., (in, jn)]\n",
    "        y0 = np.zeros((self.n, self.m, 2))\n",
    "        y0[:,:,0] = self.predator_density\n",
    "        prey_data = self.prey_density\n",
    "        for i, j in action:\n",
    "            y0[i,j,1] = 10  # place those traps at each cell\n",
    "        y0 = y0.flatten()\n",
    "\n",
    "        # get impact on predator and prey spread after placement window steps\n",
    "        master_sol = np.ndarray((self.n*self.m*2,self.pts_per_sec))\n",
    "        \n",
    "        for _ in range(self.trap_replacement_rate):\n",
    "            # trap solver, only grab single timestep\n",
    "            sol = solve_ivp(synthetic_data.spatial_dynamics_traps, y0=y0, t_span=[0,1], t_eval=np.linspace(0, 1, self.pts_per_sec), args=(self.n, self.m))\n",
    "            # prey solver y0 creation\n",
    "            y_prey = np.zeros((self.n, self.m, 2))\n",
    "            last_dim = int(self.pts_per_sec)\n",
    "            sol_use = sol.y.reshape((self.n, self.m, 2, last_dim))\n",
    "            pred_data_new, trap_data_new = sol_use[:, :, 0, :], sol_use[:, :, 1, :]\n",
    "            # set prey from timestep of interest as predator in y_prey\n",
    "            y_prey[:,:,1] = pred_data_new[:, :, -1]\n",
    "            # grab predator information from prey data\n",
    "            y_prey[:,:,0] = prey_data\n",
    "            y_prey = y_prey.flatten()\n",
    "            # prey solver, only grab single timestep\n",
    "            sol_prey = solve_ivp(synthetic_data.spatial_dynamics, y0=y_prey, t_span=[0,1], t_eval=np.linspace(0, 1, self.pts_per_sec), args=(self.n, self.m))\n",
    "            # create y0 for next run of trap solver, overwrite y0 and prey_data\n",
    "            y0 = np.zeros((self.n, self.m, 2))\n",
    "            sol_prey_use = sol_prey.y.reshape((self.n, self.m, 2, last_dim))\n",
    "            prey_data, predator_data = sol_prey_use[:, :, 0, :], sol_prey_use[:, :, 1, :]\n",
    "            y0[:,:,0] = predator_data[:, :, -1]\n",
    "            prey_data = prey_data[:, :, -1]\n",
    "            # initialize trap locations based on number of desired traps and density, re initialize per replacement time\n",
    "            y0[:,:,1] = trap_data_new[:,:,-1]\n",
    "            y0 = y0.flatten()\n",
    "            master_sol = np.concatenate((master_sol, sol_prey.y), 1)\n",
    "\n",
    "        master_sol = master_sol[:,100:].reshape(self.n, self.m,2,self.pts_per_sec*self.trap_replacement_rate)\n",
    "        \n",
    "        # Extract the last step predator and prey densities from the solution\n",
    "        self.prey_density , self.predator_density = master_sol[:, :, 0, -1], master_sol[:, :, 1, -1]\n",
    "\n",
    "        # Calculate reward based on the change in predator density\n",
    "        reward = -np.sum(self.predator_density)\n",
    "\n",
    "        # Check termination condition\n",
    "        predator_sum_zero = np.sum(self.predator_density) == 0\n",
    "        # Check if the maximum number of steps is reached\n",
    "        max_steps_reached = self.current_step >= self.max_steps\n",
    "    \n",
    "        # Combine termination conditions\n",
    "        done = predator_sum_zero or max_steps_reached\n",
    "        \n",
    "        # Return next state, reward, and done flag (assuming no termination condition for now)\n",
    "        self.current_step += 1\n",
    "        print(f\"finished step {self.current_step}\")\n",
    "        return self.predator_density, self.prey_density, reward, done\n",
    "\n",
    "# define DQN\n",
    "class QNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_actions, input_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.dense1 = torch.nn.Linear(input_size, 64)\n",
    "        self.dense2 = torch.nn.Linear(64, 64)\n",
    "        self.output_layer = torch.nn.Linear(64, num_actions)\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.nn.functional.relu(self.dense1(state))\n",
    "        x = torch.nn.functional.relu(self.dense2(x))\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Simple Q-learning algorithm with experience replay\n",
    "class QLearningAgent:\n",
    "    def __init__(self, m,n,input_size,num_traps):\n",
    "        self.num_actions = m * n\n",
    "        self.q_network = QNetwork(self.num_actions,input_size)\n",
    "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=0.001)\n",
    "        self.memory = []\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.num_traps = num_traps\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # Epsilon-greedy policy\n",
    "        if np.random.rand() < 0.1:\n",
    "            # Explore: Randomly select trap locations\n",
    "            trap_indices = []\n",
    "            for _ in range(self.num_traps):\n",
    "                trap_indices.append((np.random.randint(self.n), np.random.randint(self.m)))\n",
    "            return trap_indices\n",
    "        else:\n",
    "            # Exploit: Select actions with highest Q-values\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            q_values = self.q_network(state_tensor)\n",
    "\n",
    "            # Select the top num_traps indices with highest Q-values\n",
    "            top_indices = torch.topk(q_values, self.num_traps).indices.tolist()\n",
    "            \n",
    "            # Convert indices to trap locations\n",
    "            trap_indices = []\n",
    "            for idx in top_indices:\n",
    "                i = idx // self.m\n",
    "                j = idx % self.m\n",
    "                trap_indices.append((i, j))\n",
    "            return trap_indices\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def experience_replay(self, batch_size=32):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = np.random.choice(len(self.memory), batch_size, replace=False)\n",
    "        states, targets = [], []\n",
    "        for idx in minibatch:\n",
    "            state, action, reward, next_state, done = self.memory[idx]\n",
    "            states.append(state)\n",
    "            q_values = self.q_network(torch.tensor([state], dtype=torch.float32)).detach().numpy()[0]\n",
    "            if done:\n",
    "                q_values[action] = reward\n",
    "            else:\n",
    "                next_q_values = self.q_network(torch.tensor([next_state], dtype=torch.float32)).detach().numpy()[0]\n",
    "                q_values[action] = reward + 0.9 * np.max(next_q_values)\n",
    "            targets.append(q_values)\n",
    "        \n",
    "        states = np.array(states, dtype=np.float32)\n",
    "        targets = np.array(targets, dtype=np.float32)\n",
    "        \n",
    "        states_tensor = torch.tensor(states)\n",
    "        targets_tensor = torch.tensor(targets)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        q_values = self.q_network(states_tensor)\n",
    "        loss = torch.nn.functional.mse_loss(q_values, targets_tensor)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        \n",
    "# Training loop\n",
    "def train_agent(env, agent, num_episodes=1000):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            state = np.array(state).flatten()\n",
    "            action = agent.select_action(state)\n",
    "            next_state_prey, next_state_pred, reward, done = env.step(action)\n",
    "            next_state = np.concatenate((next_state_prey.flatten(), next_state_pred.flatten()))\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.experience_replay()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predator_locations_single(state, n):\n",
    "    predator_density, prey_density = state[0], state[1]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))  # One row, two columns\n",
    "    titles = ['Predator Grid', 'Prey Grid']\n",
    "    grids = [predator_density, prey_density]\n",
    "\n",
    "    for ax, title, grid in zip(axes, titles, grids):\n",
    "        im = ax.imshow(grid, cmap='YlGn')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Column')\n",
    "        ax.set_ylabel('Row')\n",
    "        ax.axis('on')\n",
    "        plt.colorbar(im, ax=ax, orientation='vertical', fraction=0.05, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "def predict(env, agent):\n",
    "    state = env.reset()\n",
    "    # # plot the initialized pred prey spread\n",
    "    # plot_predator_locations_single(state,env.n)\n",
    "    state = np.array(state).flatten()\n",
    "    action = agent.select_action(state)\n",
    "    print(\"Predicted trap locations for next episode:\", action)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished step 1\n",
      "finished step 2\n",
      "finished step 3\n",
      "finished step 4\n",
      "finished step 5\n",
      "finished step 6\n",
      "finished step 7\n",
      "finished step 8\n",
      "finished step 9\n",
      "finished step 10\n",
      "finished step 11\n",
      "finished step 12\n",
      "finished step 13\n",
      "finished step 14\n",
      "finished step 15\n",
      "finished step 16\n",
      "finished step 17\n",
      "finished step 18\n",
      "finished step 19\n",
      "finished step 20\n",
      "finished step 21\n",
      "finished step 22\n",
      "finished step 23\n",
      "finished step 24\n",
      "finished step 25\n",
      "finished step 26\n",
      "finished step 27\n",
      "finished step 28\n",
      "finished step 29\n",
      "finished step 30\n",
      "finished step 31\n",
      "finished step 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8y/5_v9fljd3nn234twc_p38rx40000gn/T/ipykernel_90817/3993078073.py:159: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  q_values = self.q_network(torch.tensor([state], dtype=torch.float32)).detach().numpy()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished step 33\n",
      "finished step 34\n",
      "finished step 35\n",
      "finished step 36\n",
      "finished step 37\n",
      "finished step 38\n",
      "finished step 39\n",
      "finished step 40\n",
      "finished step 41\n",
      "finished step 42\n",
      "finished step 43\n",
      "finished step 44\n",
      "finished step 45\n",
      "finished step 46\n",
      "finished step 47\n",
      "finished step 48\n",
      "finished step 49\n",
      "finished step 50\n",
      "finished step 51\n",
      "finished step 52\n",
      "finished step 53\n",
      "finished step 54\n",
      "finished step 55\n",
      "finished step 56\n",
      "finished step 57\n",
      "finished step 58\n",
      "finished step 59\n",
      "finished step 60\n",
      "finished step 61\n",
      "finished step 62\n",
      "finished step 63\n",
      "finished step 64\n",
      "finished step 65\n",
      "finished step 66\n",
      "finished step 67\n",
      "finished step 68\n",
      "finished step 69\n",
      "finished step 70\n",
      "finished step 71\n",
      "finished step 72\n",
      "finished step 73\n",
      "finished step 74\n",
      "finished step 75\n",
      "finished step 76\n",
      "finished step 77\n",
      "finished step 78\n",
      "finished step 79\n",
      "finished step 80\n",
      "finished step 81\n",
      "finished step 82\n",
      "finished step 83\n",
      "finished step 84\n",
      "finished step 85\n",
      "finished step 86\n",
      "finished step 87\n",
      "finished step 88\n",
      "finished step 89\n",
      "finished step 90\n",
      "finished step 91\n",
      "finished step 92\n",
      "finished step 93\n",
      "finished step 94\n",
      "finished step 95\n",
      "finished step 96\n",
      "finished step 97\n",
      "finished step 98\n",
      "finished step 99\n",
      "finished step 100\n",
      "finished step 101\n",
      "Episode 1/1, Total Reward: -481980.6119772124\n",
      "Predicted trap locations for next episode: [(18, 15), (23, 14), (4, 2), (20, 7), (2, 0), (2, 12), (12, 9), (12, 4), (19, 4), (7, 20), (19, 0), (23, 15), (15, 1), (14, 8), (13, 24), (21, 23), (14, 6), (9, 16), (7, 10), (1, 19)]\n"
     ]
    }
   ],
   "source": [
    "# Create environment and agent\n",
    "env = TrapEnvironment()\n",
    "agent = QLearningAgent(n=env.n, m= env.m, input_size=env.n * env.m*2, num_traps=env.num_traps)\n",
    "\n",
    "# Train the agent\n",
    "train_agent(env, agent,num_episodes=1)\n",
    "\n",
    "# Predict and display predictions in interactive slider\n",
    "trap_locations = predict(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted trap locations for next episode: [(18, 15), (23, 14), (4, 2), (20, 7), (2, 0), (2, 12), (12, 9), (12, 4), (19, 4), (7, 20), (19, 0), (23, 15), (15, 1), (14, 8), (13, 24), (14, 6), (9, 16), (21, 23), (7, 10), (1, 19)]\n",
      "(1250, 2500)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7018f0ec5c48e0804df12d7e1947c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='../Data/val_predict.npy', description='file_loc'), IntSlider(value=0, descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_predator_locations_at_timestep_here(file_loc, timestep)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# display predictions\n",
    "trap_locations = predict(env, agent)\n",
    "max_timestep = 24\n",
    "\n",
    "\n",
    "# run and save a few timesteps of trap behavior with those locations\n",
    "def generate_traps_next(env, trap_locations=trap_locations, num_traj=200, num_replacements=2, len_traj=50, pts_per_sec=100, save_loc='../Data/val_predict.npy', prey_range=(1, 5), predator_range=(1, 3)):\n",
    "    # initialization stuff\n",
    "    n = 25\n",
    "    m = 25\n",
    "    replacement_window = int(len_traj/num_replacements)\n",
    "    dataset = np.zeros((num_traj, n * m * 2, len_traj * pts_per_sec))  # That will store each simulation\n",
    "    t_span = [0, replacement_window]\n",
    "    t_eval = np.linspace(0, replacement_window, replacement_window * pts_per_sec)  # Time vector\n",
    "\n",
    "    # Change this line to configure how much you downsample the data, and the final time range\n",
    "    downsample_rate = int(len(t_eval) / (replacement_window * pts_per_sec))\n",
    "    idx = np.arange(0, len(t_eval), downsample_rate)\n",
    "\n",
    "    # Generate random initial values for prey and predator populations within the specified ranges\n",
    "    y0 = np.zeros((n, m, 2))\n",
    "\n",
    "    # Generating inital points for both populations\n",
    "    # set prey (invasive species) location at last timestep of initialization for new object where the traps will be the predators\n",
    "    y0[:,:,0] = env.predator_density\n",
    "    prey_data = env.prey_density\n",
    "\n",
    "    # initialize trap locations based on predictions\n",
    "    for i,j in trap_locations:\n",
    "        y0[i, j, 1] = 10\n",
    "    y0 = y0.flatten()\n",
    "\n",
    "    # modify a run s.t. each timestep consists of two solvers: one for traps and one for prey\n",
    "    # sol.y has shape (2 * n * m, 2500) with one row representing the prey(t) function and the other representing the predator(t)function\n",
    "    # sol_use = None\n",
    "    master_sol = np.ndarray((n*m*2,pts_per_sec))\n",
    "\n",
    "\n",
    "    for _ in range(replacement_window):\n",
    "        # trap solver, only grab single timestep\n",
    "        sol = solve_ivp(synthetic_data.spatial_dynamics_traps, y0=y0, t_span=[0,1], t_eval=np.linspace(0, 1, pts_per_sec), args=(n, m))\n",
    "        # prey solver y0 creation\n",
    "        y_prey = np.zeros((n, m, 2))\n",
    "        last_dim = int(pts_per_sec)\n",
    "        sol_use = sol.y.reshape((n, m, 2, last_dim))\n",
    "        pred_data_new, trap_data_new = sol_use[:, :, 0, :], sol_use[:, :, 1, :]\n",
    "        # set prey from timestep of interest as predator in y_prey\n",
    "        y_prey[:,:,1] = pred_data_new[:, :, -1]\n",
    "        # grab predator information from prey data\n",
    "        y_prey[:,:,0] = prey_data\n",
    "        y_prey = y_prey.flatten()\n",
    "        # prey solver, only grab single timestep\n",
    "        sol_prey = solve_ivp(synthetic_data.spatial_dynamics, y0=y_prey, t_span=[0,1], t_eval=np.linspace(0, 1, pts_per_sec), args=(n, m))\n",
    "        # create y0 for next run of trap solver, overwrite y0 and prey_data\n",
    "        y0 = np.zeros((n, m, 2))\n",
    "        sol_prey_use = sol_prey.y.reshape((n, m, 2, last_dim))\n",
    "        prey_data, predator_data = sol_prey_use[:, :, 0, :], sol_prey_use[:, :, 1, :]\n",
    "        y0[:,:,0] = predator_data[:, :, -1]\n",
    "        prey_data = prey_data[:, :, -1]\n",
    "        # initialize trap locations based on number of desired traps and density, re initialize per replacement time\n",
    "        y0[:,:,1] = trap_data_new[:,:,-1]\n",
    "        y0 = y0.flatten()\n",
    "        master_sol = np.concatenate((master_sol, sol_prey.y), 1)\n",
    "\n",
    "    # save master sol object\n",
    "    print(master_sol[:,100:].shape)\n",
    "    np.save(save_loc, master_sol[:,100:])\n",
    "\n",
    "\n",
    "def plot_predator_locations_at_timestep_here(file_loc, timestep):\n",
    "    dataset = np.load(file_loc)\n",
    "    data = dataset.reshape((25, 25, 2, 2500))\n",
    "    plot_predator_locations_here(data, timestep*50)\n",
    "\n",
    "def plot_predator_locations_here(grid, timestep):\n",
    "    #Get the min and max of all your data\n",
    "    _min, _max = np.amin(grid), np.amax(grid)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(2, 1, 1)\n",
    "    #Add the vmin and vmax arguments to set the color scale\n",
    "    ax.imshow(grid[:, :, 0, timestep], cmap=plt.cm.YlGn, vmin = _min, vmax = _max)\n",
    "    ax2 = fig.add_subplot(2, 1, 2)\n",
    "    #Add the vmin and vmax arguments to set the color scale\n",
    "    ax2.imshow(grid[:, :, 1, timestep], cmap=plt.cm.YlGn, vmin = _min, vmax = _max)\n",
    "    plt.show()\n",
    "\n",
    "generate_traps_next(env,trap_locations)\n",
    "\n",
    "# Create an interactive slider\n",
    "interact(plot_predator_locations_at_timestep_here, file_loc='../Data/val_predict.npy', timestep=IntSlider(min=0, max=max_timestep, step=1, value=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
