{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done importing\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "import synthetic_data\n",
    "# import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "print(\"done importing\")\n",
    "\n",
    "class TrapEnvironment:\n",
    "    def __init__(self, num_traps=20,max_steps=100):\n",
    "        # Initialize environment parameters\n",
    "        self.num_traps = num_traps\n",
    "        self.n = 25\n",
    "        self.m = 25\n",
    "        self.predator_density = None\n",
    "        self.prey_density = None\n",
    "        self.trap_replacement_rate = 10\n",
    "        self.pts_per_sec = 100\n",
    "        self.len_traj=50\n",
    "        self.current_step = 0\n",
    "        self.max_steps = max_steps\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment to its initial state (just predator and prey)\n",
    "        # Generate initial predator and prey densities\n",
    "        # default params for initial generation num_traj=200,len_traj=50, pts_per_sec=100, save_loc='../Data/val.npy', prey_range=(1, 5), predator_range=(1, 3)\n",
    "        data_init = synthetic_data.generate().reshape(self.n, self.m, 2, self.pts_per_sec*self.len_traj)\n",
    "        self.prey_density, self.predator_density = data_init[:, :, 0, -1], data_init[:, :, 1, -1]\n",
    "        return self.predator_density, self.prey_density\n",
    "\n",
    "    def step(self, action):\n",
    "        # Take action (place traps) and observe the next state and reward\n",
    "        # Update predator and prey densities based on the action\n",
    "        # Calculate reward based on the change in predator density\n",
    "        # Return next state, reward, and done flag\n",
    "\n",
    "        # Simulate predator dynamics with traps placed at specified locations\n",
    "        # Here, action is a list of trap locations [(i1, j1), (i2, j2), ..., (in, jn)]\n",
    "        y0 = np.zeros((self.n, self.m, 2))\n",
    "        y0[:,:,0] = self.predator_density\n",
    "        prey_data = self.prey_density\n",
    "        for i, j in action:\n",
    "            y0[i,j,1] = 10  # place those traps at each cell\n",
    "        y0 = y0.flatten()\n",
    "\n",
    "        # get impact on predator and prey spread after placement window steps\n",
    "        master_sol = np.ndarray((self.n*self.m*2,self.pts_per_sec))\n",
    "        \n",
    "        for _ in range(self.trap_replacement_rate):\n",
    "            # trap solver, only grab single timestep\n",
    "            sol = solve_ivp(synthetic_data.spatial_dynamics_traps, y0=y0, t_span=[0,1], t_eval=np.linspace(0, 1, self.pts_per_sec), args=(self.n, self.m))\n",
    "            # prey solver y0 creation\n",
    "            y_prey = np.zeros((self.n, self.m, 2))\n",
    "            last_dim = int(self.pts_per_sec)\n",
    "            sol_use = sol.y.reshape((self.n, self.m, 2, last_dim))\n",
    "            pred_data_new, trap_data_new = sol_use[:, :, 0, :], sol_use[:, :, 1, :]\n",
    "            \n",
    "            # set prey from timestep of interest as predator in y_prey\n",
    "            y_prey[:,:,1] = pred_data_new[:, :, -1]\n",
    "            # grab predator information from prey data\n",
    "            y_prey[:,:,0] = prey_data\n",
    "            print(y_prey.shape, \"yellow\")\n",
    "            y_prey = y_prey.flatten()\n",
    "            # prey solver, only grab single timestep\n",
    "            sol_prey = solve_ivp(synthetic_data.spatial_dynamics, y0=y_prey, t_span=[0,1], t_eval=np.linspace(0, 1, self.pts_per_sec), args=(self.n, self.m))\n",
    "            # create y0 for next run of trap solver, overwrite y0 and prey_data\n",
    "            y0 = np.zeros((self.n, self.m, 2))\n",
    "            sol_prey_use = sol_prey.y.reshape((self.n, self.m, 2, last_dim))\n",
    "            prey_data, predator_data = sol_prey_use[:, :, 0, :], sol_prey_use[:, :, 1, :]\n",
    "            y0[:,:,0] = predator_data[:, :, -1]\n",
    "            prey_data = prey_data[:, :, -1]\n",
    "            # initialize trap locations based on number of desired traps and density, re initialize per replacement time\n",
    "            y0[:,:,1] = trap_data_new[:,:,-1]\n",
    "            y0 = y0.flatten()\n",
    "            master_sol = np.concatenate((master_sol, sol_prey.y), 1)\n",
    "\n",
    "        master_sol = master_sol[:,100:].reshape(self.n, self.m,2,self.pts_per_sec*self.trap_replacement_rate)\n",
    "        \n",
    "        # Extract the last step predator and prey densities from the solution\n",
    "        self.prey_density , self.predator_density = master_sol[:, :, 0, -1], master_sol[:, :, 1, -1]\n",
    "\n",
    "        # Calculate reward based on the change in predator density\n",
    "        reward = -np.sum(self.predator_density)\n",
    "\n",
    "        # Check termination condition\n",
    "        predator_sum_zero = np.sum(self.predator_density) == 0\n",
    "        # Check if the maximum number of steps is reached\n",
    "        max_steps_reached = self.current_step >= self.max_steps\n",
    "    \n",
    "        # Combine termination conditions\n",
    "        done = predator_sum_zero or max_steps_reached\n",
    "        \n",
    "        # Return next state, reward, and done flag (assuming no termination condition for now)\n",
    "        self.current_step += 1\n",
    "        print(f\"finished step {self.current_step}\")\n",
    "        return self.predator_density, self.prey_density, reward, done\n",
    "\n",
    "# define DQN\n",
    "class QNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_actions, input_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.dense1 = torch.nn.Linear(input_size, 64)\n",
    "        self.dense2 = torch.nn.Linear(64, 64)\n",
    "        self.output_layer = torch.nn.Linear(64, num_actions)\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.nn.functional.relu(self.dense1(state))\n",
    "        x = torch.nn.functional.relu(self.dense2(x))\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Simple Q-learning algorithm with experience replay\n",
    "class QLearningAgent:\n",
    "    def __init__(self, m,n,input_size,num_traps):\n",
    "        self.num_actions = m * n\n",
    "        self.q_network = QNetwork(self.num_actions,input_size)\n",
    "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=0.001)\n",
    "        self.memory = []\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.num_traps = num_traps\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # Epsilon-greedy policy\n",
    "        if np.random.rand() < 0.1:\n",
    "            # Explore: Randomly select trap locations\n",
    "            trap_indices = []\n",
    "            for _ in range(self.num_traps):\n",
    "                trap_indices.append((np.random.randint(self.n), np.random.randint(self.m)))\n",
    "            return trap_indices\n",
    "        else:\n",
    "            # Exploit: Select actions with highest Q-values\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            q_values = self.q_network(state_tensor)\n",
    "\n",
    "            # Select the top num_traps indices with highest Q-values\n",
    "            top_indices = torch.topk(q_values, self.num_traps).indices.tolist()\n",
    "            \n",
    "            # Convert indices to trap locations\n",
    "            trap_indices = []\n",
    "            for idx in top_indices:\n",
    "                i = idx // self.m\n",
    "                j = idx % self.m\n",
    "                trap_indices.append((i, j))\n",
    "            return trap_indices\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def experience_replay(self, batch_size=32):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = np.random.choice(len(self.memory), batch_size, replace=False)\n",
    "        states, targets = [], []\n",
    "        for idx in minibatch:\n",
    "            state, action, reward, next_state, done = self.memory[idx]\n",
    "            states.append(state)\n",
    "            q_values = self.q_network(torch.tensor([state], dtype=torch.float32)).detach().numpy()[0]\n",
    "            if done:\n",
    "                q_values[action] = reward\n",
    "            else:\n",
    "                next_q_values = self.q_network(torch.tensor([next_state], dtype=torch.float32)).detach().numpy()[0]\n",
    "                q_values[action] = reward + 0.9 * np.max(next_q_values)\n",
    "            targets.append(q_values)\n",
    "        \n",
    "        states = np.array(states, dtype=np.float32)\n",
    "        targets = np.array(targets, dtype=np.float32)\n",
    "        \n",
    "        states_tensor = torch.tensor(states)\n",
    "        targets_tensor = torch.tensor(targets)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        q_values = self.q_network(states_tensor)\n",
    "        loss = torch.nn.functional.mse_loss(q_values, targets_tensor)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        \n",
    "# Training loop\n",
    "def train_agent(env, agent, num_episodes=1000):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            state = np.array(state).flatten()\n",
    "            action = agent.select_action(state)\n",
    "            next_state_prey, next_state_pred, reward, done = env.step(action)\n",
    "            next_state = np.concatenate((next_state_prey.flatten(), next_state_pred.flatten()))\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.experience_replay()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predator_locations_single(state, n):\n",
    "    predator_density, prey_density = state[0], state[1]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))  # One row, two columns\n",
    "    titles = ['Predator Grid', 'Prey Grid']\n",
    "    grids = [predator_density, prey_density]\n",
    "\n",
    "    for ax, title, grid in zip(axes, titles, grids):\n",
    "        im = ax.imshow(grid, cmap='YlGn')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Column')\n",
    "        ax.set_ylabel('Row')\n",
    "        ax.axis('on')\n",
    "        plt.colorbar(im, ax=ax, orientation='vertical', fraction=0.05, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "def predict(env, agent):\n",
    "    state = env.reset()\n",
    "    # # plot the initialized pred prey spread\n",
    "    # plot_predator_locations_single(state,env.n)\n",
    "    state = np.array(state).flatten()\n",
    "    action = agent.select_action(state)\n",
    "    print(\"Predicted trap locations for next episode:\", action)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 25, 2) yellow\n",
      "(25, 25, 2) yellow\n",
      "(25, 25, 2) yellow\n",
      "(25, 25, 2) yellow\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-46e39a138c56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Train the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Predict and display predictions in interactive slider\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-a49bc93e7df7>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(env, agent, num_episodes)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mnext_state_prey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state_prey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-a49bc93e7df7>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0my_prey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_prey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# prey solver, only grab single timestep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0msol_prey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolve_ivp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynthetic_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial_dynamics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_prey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_span\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpts_per_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;31m# create y0 for next run of trap solver, overwrite y0 and prey_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/scipy/integrate/_ivp/ivp.py\u001b[0m in \u001b[0;36msolve_ivp\u001b[0;34m(fun, t_span, y0, method, t_eval, dense_output, events, vectorized, args, **options)\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMETHODS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m     \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt_eval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/scipy/integrate/_ivp/rk.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fun, t0, y0, t_bound, max_step, rtol, atol, vectorized, first_step, **extraneous)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfirst_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             self.h_abs = select_initial_step(\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 self.error_estimator_order, self.rtol, self.atol)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/scipy/integrate/_ivp/common.py\u001b[0m in \u001b[0;36mselect_initial_step\u001b[0;34m(fun, t0, y0, f0, direction, order, rtol, atol)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdirection\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0md2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/scipy/integrate/_ivp/base.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(t, y)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnfev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/scipy/integrate/_ivp/base.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(t, y)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/scipy/integrate/_ivp/ivp.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t, x, fun)\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuggestion_tuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'jac'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/288_invasive_species/Model_Generation/synthetic_data.py\u001b[0m in \u001b[0;36mspatial_dynamics\u001b[0;34m(t, y, n, m)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# Computing growth of both groups with respect to competition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mgrowth_prey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mgrowth_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgrowth_prey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create environment and agent\n",
    "env = TrapEnvironment()\n",
    "agent = QLearningAgent(n=env.n, m= env.m, input_size=env.n * env.m*2, num_traps=env.num_traps)\n",
    "\n",
    "# Train the agent\n",
    "train_agent(env, agent,num_episodes=1)\n",
    "\n",
    "# Predict and display predictions in interactive slider\n",
    "trap_locations = predict(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted trap locations for next episode: [(18, 15), (23, 14), (4, 2), (20, 7), (2, 0), (2, 12), (12, 9), (12, 4), (19, 4), (7, 20), (19, 0), (23, 15), (15, 1), (14, 8), (13, 24), (14, 6), (9, 16), (21, 23), (7, 10), (1, 19)]\n",
      "(1250, 2500)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7018f0ec5c48e0804df12d7e1947c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='../Data/val_predict.npy', description='file_loc'), IntSlider(value=0, descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_predator_locations_at_timestep_here(file_loc, timestep)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# display predictions\n",
    "trap_locations = predict(env, agent)\n",
    "max_timestep = 24\n",
    "\n",
    "\n",
    "# run and save a few timesteps of trap behavior with those locations\n",
    "def generate_traps_next(env, trap_locations=trap_locations, num_traj=200, num_replacements=2, len_traj=50, pts_per_sec=100, save_loc='../Data/val_predict.npy', prey_range=(1, 5), predator_range=(1, 3)):\n",
    "    # initialization stuff\n",
    "    n = 25\n",
    "    m = 25\n",
    "    replacement_window = int(len_traj/num_replacements)\n",
    "    dataset = np.zeros((num_traj, n * m * 2, len_traj * pts_per_sec))  # That will store each simulation\n",
    "    t_span = [0, replacement_window]\n",
    "    t_eval = np.linspace(0, replacement_window, replacement_window * pts_per_sec)  # Time vector\n",
    "\n",
    "    # Change this line to configure how much you downsample the data, and the final time range\n",
    "    downsample_rate = int(len(t_eval) / (replacement_window * pts_per_sec))\n",
    "    idx = np.arange(0, len(t_eval), downsample_rate)\n",
    "\n",
    "    # Generate random initial values for prey and predator populations within the specified ranges\n",
    "    y0 = np.zeros((n, m, 2))\n",
    "\n",
    "    # Generating inital points for both populations\n",
    "    # set prey (invasive species) location at last timestep of initialization for new object where the traps will be the predators\n",
    "    y0[:,:,0] = env.predator_density\n",
    "    prey_data = env.prey_density\n",
    "\n",
    "    # initialize trap locations based on predictions\n",
    "    for i,j in trap_locations:\n",
    "        y0[i, j, 1] = 10\n",
    "    y0 = y0.flatten()\n",
    "\n",
    "    # modify a run s.t. each timestep consists of two solvers: one for traps and one for prey\n",
    "    # sol.y has shape (2 * n * m, 2500) with one row representing the prey(t) function and the other representing the predator(t)function\n",
    "    # sol_use = None\n",
    "    master_sol = np.ndarray((n*m*2,pts_per_sec))\n",
    "\n",
    "\n",
    "    for _ in range(replacement_window):\n",
    "        # trap solver, only grab single timestep\n",
    "        sol = solve_ivp(synthetic_data.spatial_dynamics_traps, y0=y0, t_span=[0,1], t_eval=np.linspace(0, 1, pts_per_sec), args=(n, m))\n",
    "        # prey solver y0 creation\n",
    "        y_prey = np.zeros((n, m, 2))\n",
    "        last_dim = int(pts_per_sec)\n",
    "        sol_use = sol.y.reshape((n, m, 2, last_dim))\n",
    "        pred_data_new, trap_data_new = sol_use[:, :, 0, :], sol_use[:, :, 1, :]\n",
    "        # set prey from timestep of interest as predator in y_prey\n",
    "        y_prey[:,:,1] = pred_data_new[:, :, -1]\n",
    "        # grab predator information from prey data\n",
    "        y_prey[:,:,0] = prey_data\n",
    "        y_prey = y_prey.flatten()\n",
    "        # prey solver, only grab single timestep\n",
    "        sol_prey = solve_ivp(synthetic_data.spatial_dynamics, y0=y_prey, t_span=[0,1], t_eval=np.linspace(0, 1, pts_per_sec), args=(n, m))\n",
    "        # create y0 for next run of trap solver, overwrite y0 and prey_data\n",
    "        y0 = np.zeros((n, m, 2))\n",
    "        sol_prey_use = sol_prey.y.reshape((n, m, 2, last_dim))\n",
    "        prey_data, predator_data = sol_prey_use[:, :, 0, :], sol_prey_use[:, :, 1, :]\n",
    "        y0[:,:,0] = predator_data[:, :, -1]\n",
    "        prey_data = prey_data[:, :, -1]\n",
    "        # initialize trap locations based on number of desired traps and density, re initialize per replacement time\n",
    "        y0[:,:,1] = trap_data_new[:,:,-1]\n",
    "        y0 = y0.flatten()\n",
    "        master_sol = np.concatenate((master_sol, sol_prey.y), 1)\n",
    "\n",
    "    # save master sol object\n",
    "    print(master_sol[:,100:].shape)\n",
    "    np.save(save_loc, master_sol[:,100:])\n",
    "\n",
    "\n",
    "def plot_predator_locations_at_timestep_here(file_loc, timestep):\n",
    "    dataset = np.load(file_loc)\n",
    "    data = dataset.reshape((25, 25, 2, 2500))\n",
    "    plot_predator_locations_here(data, timestep*50)\n",
    "\n",
    "def plot_predator_locations_here(grid, timestep):\n",
    "    #Get the min and max of all your data\n",
    "    _min, _max = np.amin(grid), np.amax(grid)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(2, 1, 1)\n",
    "    #Add the vmin and vmax arguments to set the color scale\n",
    "    ax.imshow(grid[:, :, 0, timestep], cmap=plt.cm.YlGn, vmin = _min, vmax = _max)\n",
    "    ax2 = fig.add_subplot(2, 1, 2)\n",
    "    #Add the vmin and vmax arguments to set the color scale\n",
    "    ax2.imshow(grid[:, :, 1, timestep], cmap=plt.cm.YlGn, vmin = _min, vmax = _max)\n",
    "    plt.show()\n",
    "\n",
    "generate_traps_next(env,trap_locations)\n",
    "\n",
    "# Create an interactive slider\n",
    "interact(plot_predator_locations_at_timestep_here, file_loc='../Data/val_predict.npy', timestep=IntSlider(min=0, max=max_timestep, step=1, value=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "365bf65fb91c106cdbd1c0bc03ff0be5109c6b4cf462d087395592db3f17fbb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
