{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done importing\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "import synthetic_data\n",
    "# import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "print(\"done importing\")\n",
    "\n",
    "class TrapEnvironment:\n",
    "    def __init__(self, num_traps=20,max_steps=100):\n",
    "        # Initialize environment parameters\n",
    "        self.num_traps = num_traps\n",
    "        self.n = 25\n",
    "        self.m = 25\n",
    "        self.predator_density = None\n",
    "        self.prey_density = None\n",
    "        self.trap_replacement_rate = 10\n",
    "        self.pts_per_sec = 100\n",
    "        self.len_traj=50\n",
    "        self.current_step = 0\n",
    "        self.max_steps = max_steps\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment to its initial state (just predator and prey)\n",
    "        # Generate initial predator and prey densities\n",
    "        # default params for initial generation num_traj=200,len_traj=50, pts_per_sec=100, save_loc='../Data/val.npy', prey_range=(1, 5), predator_range=(1, 3)\n",
    "        data_init = synthetic_data.generate().reshape(self.n, self.m, 2, self.pts_per_sec*self.len_traj)\n",
    "        self.prey_density, self.predator_density = data_init[:, :, 0, -1], data_init[:, :, 1, -1]\n",
    "        return self.predator_density, self.prey_density\n",
    "\n",
    "    def step(self, action):\n",
    "        # Take action (place traps) and observe the next state and reward\n",
    "        # Update predator and prey densities based on the action\n",
    "        # Calculate reward based on the change in predator density\n",
    "        # Return next state, reward, and done flag\n",
    "\n",
    "        # Simulate predator dynamics with traps placed at specified locations\n",
    "        # Here, action is a list of trap locations [(i1, j1), (i2, j2), ..., (in, jn)]\n",
    "        y0 = np.zeros((self.n, self.m, 2))\n",
    "        for i, j in action:\n",
    "            print(i)\n",
    "            print(j)\n",
    "            y0[i,j,1] = 10  # place those traps at each cell\n",
    "        y0 = y0.flatten()\n",
    "\n",
    "        # get impact on predator and prey spread after placement window steps\n",
    "        master_sol = np.ndarray((self.n*self.m*2,self.pts_per_sec))\n",
    "        \n",
    "        for _ in range(self.trap_replacement_rate):\n",
    "            # trap solver, only grab single timestep\n",
    "            sol = solve_ivp(synthetic_data.spatial_dynamics_traps, y0=y0, t_span=[0,1], t_eval=np.linspace(0, 1, self.pts_per_sec), args=(self.n, self.m))\n",
    "            # prey solver y0 creation\n",
    "            y_prey = np.zeros((self.n, self.m, 2))\n",
    "            last_dim = int(self.pts_per_sec)\n",
    "            sol_use = sol.y.reshape((self.n, self.m, 2, last_dim))\n",
    "            pred_data_new, trap_data_new = sol_use[:, :, 0, :], sol_use[:, :, 1, :]\n",
    "            # set prey from timestep of interest as predator in y_prey\n",
    "            y_prey[:,:,1] = pred_data_new[:, :, -1]\n",
    "            # grab predator information from prey data\n",
    "            y_prey[:,:,0] = self.prey_density\n",
    "            y_prey = y_prey.flatten()\n",
    "            # prey solver, only grab single timestep\n",
    "            sol_prey = solve_ivp(synthetic_data.spatial_dynamics, y0=y_prey, t_span=[0,1], t_eval=np.linspace(0, 1, self.pts_per_sec), args=(self.n, self.m))\n",
    "            # create y0 for next run of trap solver, overwrite y0 and prey_data\n",
    "            y0 = np.zeros((self.n, self.m, 2))\n",
    "            sol_prey_use = sol_prey.y.reshape((self.n, self.m, 2, last_dim))\n",
    "            self.prey_density, self.predator_density = sol_prey_use[:, :, 0, :], sol_prey_use[:, :, 1, :]\n",
    "            y0[:,:,0] = self.predator_density[:, :, -1]\n",
    "            self.prey_density = self.prey_density[:, :, -1]\n",
    "            # initialize trap locations based on number of desired traps and density, re initialize per replacement time\n",
    "            y0[:,:,1] = trap_data_new[:,:,-1]\n",
    "            y0 = y0.flatten()\n",
    "            master_sol = np.concatenate((master_sol, sol_prey.y), 1)\n",
    "\n",
    "        master_sol = master_sol[:,100:].reshape(self.n, self.m,2,self.pts_per_sec*self.trap_replacement_rate)\n",
    "        \n",
    "        # Extract the last step predator and prey densities from the solution\n",
    "        self.prey_density , self.predator_density = master_sol[:, :, 0, -1], master_sol[:, :, 1, -1]\n",
    "\n",
    "        # Calculate reward based on the change in predator density\n",
    "        reward = -np.sum(self.predator_density)\n",
    "\n",
    "        # Check termination condition\n",
    "        predator_sum_zero = np.sum(self.predator_density) == 0\n",
    "        # Check if the maximum number of steps is reached\n",
    "        max_steps_reached = self.current_step >= self.max_steps\n",
    "    \n",
    "        # Combine termination conditions\n",
    "        done = predator_sum_zero or max_steps_reached\n",
    "        \n",
    "        # Return next state, reward, and done flag (assuming no termination condition for now)\n",
    "        self.current_step += 1\n",
    "        return self.predator_density, self.prey_density, reward, False\n",
    "\n",
    "# define DQN\n",
    "class QNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_actions, input_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.dense1 = torch.nn.Linear(input_size, 64)\n",
    "        self.dense2 = torch.nn.Linear(64, 64)\n",
    "        self.output_layer = torch.nn.Linear(64, num_actions)\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.nn.functional.relu(self.dense1(state))\n",
    "        x = torch.nn.functional.relu(self.dense2(x))\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Simple Q-learning algorithm with experience replay\n",
    "class QLearningAgent:\n",
    "    def __init__(self, m,n,input_size):\n",
    "        self.num_actions = m * n\n",
    "        self.q_network = QNetwork(self.num_actions,input_size)\n",
    "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=0.001)\n",
    "        self.memory = []\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # Epsilon-greedy policy\n",
    "        if np.random.rand() < 0.1:\n",
    "            # Explore: Randomly select a trap location\n",
    "            return [(np.random.randint(self.n), np.random.randint(self.m))]\n",
    "        else:\n",
    "            # Exploit: Select action with highest Q-value\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)#.view(1, -1)  # Reshape state to (1, input_size)\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            action_index = torch.argmax(q_values).item()\n",
    "            # Convert action index to trap location\n",
    "            i = action_index // self.m\n",
    "            j = action_index % self.m\n",
    "            return [(i, j)]\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def experience_replay(self, batch_size=32):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = np.random.choice(len(self.memory), batch_size, replace=False)\n",
    "        states, targets = [], []\n",
    "        for idx in minibatch:\n",
    "            state, action, reward, next_state, done = self.memory[idx]\n",
    "            states.append(state)\n",
    "            q_values = self.q_network(torch.tensor([state], dtype=torch.float32)).detach().numpy()[0]\n",
    "            if done:\n",
    "                q_values[action] = reward\n",
    "            else:\n",
    "                next_q_values = self.q_network(torch.tensor([next_state], dtype=torch.float32)).detach().numpy()[0]\n",
    "                q_values[action] = reward + 0.9 * np.max(next_q_values)\n",
    "            targets.append(q_values)\n",
    "        \n",
    "        states = np.array(states, dtype=np.float32)\n",
    "        targets = np.array(targets, dtype=np.float32)\n",
    "        \n",
    "        states_tensor = torch.tensor(states)\n",
    "        targets_tensor = torch.tensor(targets)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        q_values = self.q_network(states_tensor)\n",
    "        loss = torch.nn.functional.mse_loss(q_values, targets_tensor)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        \n",
    "# Training loop\n",
    "def train_agent(env, agent, num_episodes=1000):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            state = np.array(state).flatten()\n",
    "            print(state.shape)\n",
    "            action = agent.select_action(state)\n",
    "            next_state_prey, next_state_pred, reward, done = env.step(action)\n",
    "            next_state = np.concatenate((next_state_prey.flatten(), next_state_pred.flatten()))\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.experience_replay()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250,)\n",
      "0\n",
      "23\n",
      "(1250,)\n",
      "0\n",
      "23\n",
      "(1250,)\n",
      "0\n",
      "23\n",
      "(1250,)\n",
      "0\n",
      "23\n",
      "(1250,)\n",
      "0\n",
      "23\n",
      "(1250,)\n",
      "0\n",
      "23\n",
      "(1250,)\n",
      "0\n",
      "23\n",
      "(1250,)\n",
      "20\n",
      "0\n",
      "(1250,)\n",
      "0\n",
      "23\n",
      "(1250,)\n",
      "0\n",
      "23\n",
      "(1250,)\n",
      "0\n",
      "23\n",
      "(1250,)\n",
      "0\n",
      "23\n",
      "(1250,)\n",
      "0\n",
      "23\n",
      "(1250,)\n",
      "0\n",
      "23\n",
      "(1250,)\n",
      "0\n",
      "23\n",
      "(1250,)\n",
      "5\n",
      "7\n",
      "(1250,)\n",
      "0\n",
      "23\n",
      "(1250,)\n",
      "6\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Create environment and agent\n",
    "env = TrapEnvironment()\n",
    "agent = QLearningAgent(n=env.n, m= env.m, input_size=env.n * env.m*2)\n",
    "\n",
    "# Train the agent\n",
    "train_agent(env, agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
